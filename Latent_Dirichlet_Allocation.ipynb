{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/scojocariu/.local/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/PP/lib/python3.6/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: tqdm in /Users/scojocariu/.local/lib/python3.6/site-packages (from nltk) (4.54.1)\n",
      "Requirement already satisfied: click in /Users/scojocariu/.local/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /Users/scojocariu/.local/lib/python3.6/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: wikipedia in /opt/anaconda3/envs/PP/lib/python3.6/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/PP/lib/python3.6/site-packages (from wikipedia) (4.9.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/anaconda3/envs/PP/lib/python3.6/site-packages (from wikipedia) (2.24.0)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /opt/anaconda3/envs/PP/lib/python3.6/site-packages (from beautifulsoup4->wikipedia) (2.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/envs/PP/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/envs/PP/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/PP/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/PP/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install wikipedia\n",
    "from scipy.special import softmax\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import wikipedia\n",
    "from nltk.corpus import stopwords \n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download documents from the Wikipedia, I created a class called DocumentExtractor that downloads the content of a wikipedia document based on its title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentExtractor():\n",
    "    def __init__(self, title):\n",
    "        self.title = title\n",
    "        self.text = wikipedia.page(self.title).summary\n",
    "    def getContent(self):\n",
    "        return self.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I implemented a class called LatentDirichletAllocation that receives: no_topics: number of topics (hyper-parameter), initial corpus: the corpus containing the documents, alpha: $\\theta _{i}\\sim \\operatorname {Dir} (\\alpha )$, beta: $\\varphi _{k}\\sim \\operatorname {Dir} (\\beta )$. It converts all the characters to lower-case, uses a stemmer internaly to reduce the vocabulary size, and instatiates all the variables needed by the pymc model to create a LDA. There are methods to deal with unseen documents, while plotting different useful metrics: two types of predictions (using a heuristical approach and a statistical one), two types of topic-similarity (Jensen-Shannon and Kullback–Leibler).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDirichletAllocation():\n",
    "    def __init__(self, no_topics, initial_corpus, alpha=1, beta=1, show_tokens=True, show_idxTokens=True):\n",
    "        \n",
    "        # stemmer to reduce the size of the vocabulary\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        self.stop_words = set(stopwords.words('english')) \n",
    "        \n",
    "        self.no_topics = no_topics\n",
    "        self.initialCorpus = initial_corpus\n",
    "        \n",
    "        # preprocess the corpus, converts it to tokens, applies a stemming process and removes stop-words.\n",
    "        # it returns the corpus as words, the corpus containing indexes as words,\n",
    "        # the wordToIdx dictionary and the idxToWord dictionary\n",
    "        self.toTokens_corpus, self.toIdx_corpus, self.wordsToIdx, self.idxToWords = self.preprocess(initial_corpus)\n",
    "        \n",
    "        if show_tokens:\n",
    "            print(\"To tokens corpus (tokens):\\n\")\n",
    "            for x in self.toTokens_corpus:\n",
    "                print(\"{}\\n\".format(x))\n",
    "                \n",
    "        if show_idxTokens:\n",
    "            print(\"\\nTo idx corpus:\\n\")\n",
    "            for x in self.toIdx_corpus:\n",
    "                print(\"{}\\n\".format(x))\n",
    "        \n",
    "        self.no_words = len(self.wordsToIdx)\n",
    "        self.no_documents = len(self.toIdx_corpus)\n",
    "        \n",
    "        # if alpha is a number, it instantiate a list of alpha's (of size self.no_topics)\n",
    "        # else alpha_list = alpha (it assumes alpha is a list containing the alpha's values)\n",
    "        alpha_list, beta_list = [], []\n",
    "        if isinstance(alpha, list):\n",
    "            alpha_list = alpha\n",
    "        else:\n",
    "            alpha_list = np.array([alpha for _ in range(self.no_topics)])\n",
    "        \n",
    "        # the same process for alpha\n",
    "        if isinstance(beta, list):\n",
    "            beta_list = beta\n",
    "        else:\n",
    "            beta_list = np.array([beta for _ in range(self.no_words)])\n",
    "        \n",
    "        # calculate the length of each document\n",
    "        self.doc_lengths_list = []\n",
    "        for i in range(self.no_documents):\n",
    "            self.doc_lengths_list.append(len(self.toIdx_corpus[i]))\n",
    "        \n",
    "        # create a CompletedDirichlet of size alpha_list variable for each document in the corpus\n",
    "        # there is a need for both the lists to be passed to the mcmc model. (As giving only a reference\n",
    "        # to a pm.CompletedDirichlet doesn't create the proper graph containing the pm.Dirichlet)\n",
    "        self.theta_list_, self.theta_list = [], []\n",
    "        for i in range(self.no_documents):\n",
    "            aux = pm.Dirichlet(\"theta_{}\".format(i), theta=alpha_list)\n",
    "            self.theta_list_.append(aux)\n",
    "            \n",
    "            aux = pm.CompletedDirichlet(\"ctheta_{}\".format(i), aux)\n",
    "            self.theta_list.append(aux)\n",
    "            \n",
    "        # add theta_list_ and theta_list inside a container\n",
    "        self.theta_list_ = pm.Container(self.theta_list_)\n",
    "        self.theta_list = pm.Container(self.theta_list)\n",
    "\n",
    "        # create a CompletedDirichlet of size beta_list variable for each topic available\n",
    "        # The same remark as before\n",
    "        self.phi_list_, self.phi_list = [], []\n",
    "        for i in range(self.no_topics):\n",
    "            aux = pm.Dirichlet(\"phi_{}\".format(i), theta=beta_list)\n",
    "            self.phi_list_.append(aux)\n",
    "            \n",
    "            aux = pm.CompletedDirichlet(\"cphi_{}\".format(i), aux)\n",
    "            self.phi_list.append(aux)\n",
    "        \n",
    "        # add phi_list_ and phi_list inside a container\n",
    "        self.phi_list_ = pm.Container(self.phi_list_)\n",
    "        self.phi_list = pm.Container(self.phi_list)\n",
    "        \n",
    "        \n",
    "        # create a categorical variable for each word in the corpus, with p = self.theta_list[i][j]\n",
    "        self.z_list_, self.z_list = [], []\n",
    "        for i in range(self.no_documents):\n",
    "            aux_list = []\n",
    "            for j in range(self.doc_lengths_list[i]):\n",
    "                aux = pm.Categorical(\"z_{}-{}\".format(i, j), \n",
    "                                     p=self.theta_list[i],\n",
    "                                     value=np.random.randint(self.no_topics))\n",
    "                aux_list.append(aux)\n",
    "                self.z_list_.append(aux)\n",
    "            self.z_list.append(pm.Container(aux_list))\n",
    "        # add z_list_ and z_list inside a container\n",
    "        self.z_list_ = pm.Container(self.z_list_)\n",
    "        self.z_list = pm.Container(self.z_list)\n",
    "        \n",
    "        \n",
    "        # create a categorical variable for each word in the corpus, with p = self.theta_list[i][j]\n",
    "        # pm.Lambda is used here to simulate self.phi_list[self.z_list[i][j]] (as it cannot be called like this directly)\n",
    "        # OBS:  value is set to the actual word in the corpus self.toIdx_corpus[i][j], while setting observed=True \n",
    "        # (to allow learning)\n",
    "        self.w_list = []\n",
    "        for i in range(self.no_documents):\n",
    "            for j in range(self.doc_lengths_list[i]):\n",
    "                aux = pm.Categorical(\"w_{}-{}\".format(i, j), \n",
    "                                     p=pm.Lambda(\"lambda_{}-{}\".format(i, j), lambda z=self.z_list[i][j], phi_list=self.phi_list: phi_list[z]), \n",
    "                                     value=self.toIdx_corpus[i][j], \n",
    "                                     observed=True)\n",
    "                self.w_list.append(aux)\n",
    "        # add w_list inside a container   \n",
    "        self.w_list = pm.Container(self.w_list)\n",
    "        \n",
    "        # add all the variables created inside the model\n",
    "        self.model = pm.Model([self.theta_list_, self.theta_list, self.phi_list_, self.phi_list, self.z_list_, self.z_list, self.w_list])\n",
    "        self.mcmc = pm.MCMC(self.model)\n",
    "    \n",
    "    # if run the mcmc model, setting no_iter, burn iters and burn_till_tuned iter\n",
    "    def fit(self, no_iter, burn, burn_till_tuned, extraction_type=\"mean\"):\n",
    "        self.mcmc.sample(no_iter, burn=burn, burn_till_tuned=burn_till_tuned)\n",
    "        \n",
    "        if extraction_type == \"last\":\n",
    "            self.theta_extracted = []\n",
    "            for i in range(self.no_documents):\n",
    "                self.theta_extracted.append(self.mcmc.trace(\"ctheta_{}\".format(i))[-1])\n",
    "            self.theta_extracted = np.squeeze(np.asarray(self.theta_extracted))\n",
    "\n",
    "            self.phi_extracted = []\n",
    "            for i in range(self.no_topics):\n",
    "                self.phi_extracted.append(self.mcmc.trace(\"cphi_{}\".format(i))[-1])\n",
    "            self.phi_extracted = np.squeeze(np.asarray(self.phi_extracted))   \n",
    "        \n",
    "        elif extraction_type == \"mean\":\n",
    "            self.theta_extracted = []\n",
    "            for i in range(self.no_documents):\n",
    "                self.theta_extracted.append(np.mean(self.mcmc.trace(\"ctheta_{}\".format(i))[-1000:], axis=0))\n",
    "            self.theta_extracted = np.squeeze(np.asarray(self.theta_extracted))\n",
    "\n",
    "            self.phi_extracted = []\n",
    "            for i in range(self.no_topics):\n",
    "                self.phi_extracted.append(np.mean(self.mcmc.trace(\"cphi_{}\".format(i))[-1000:], axis=0))\n",
    "            self.phi_extracted = np.squeeze(np.asarray(self.phi_extracted)) \n",
    "            \n",
    "        \n",
    "    # return for each topic the words that are the most representative (each word appear in exact one topic)\n",
    "    def getRepresentativeWordsForTopics(self, show_words=True):\n",
    "        \n",
    "        # instatiate a dictionary of lists of size self.no_topics \n",
    "        d = {}\n",
    "        for i in range(self.no_topics):\n",
    "            d[\"topic_{}\".format(i)] = []\n",
    "        \n",
    "        # get the max value for each word across the topics\n",
    "        word_topics_idx = np.argmax(self.phi_extracted, axis=0)\n",
    "        \n",
    "        assert(len(word_topics_idx) == len(self.idxToWords))\n",
    "        \n",
    "        # add each word to their corresponding place\n",
    "        for i in range(len(word_topics_idx)):\n",
    "            d[\"topic_{}\".format(word_topics_idx[i])].append(self.idxToWords[i])\n",
    "        \n",
    "        if show_words:\n",
    "            print(\"Representative words for each topic:\")\n",
    "            for key in d:\n",
    "                print(\"{} : {}\\n\".format(key, d[key]))\n",
    "            print()\n",
    "        \n",
    "        return d\n",
    "        \n",
    "    \n",
    "    # receives a document (unseen), converts it to lower-case, split it into words (removing special characters)\n",
    "    # and stemmatize them + removes the stop-words (all this steps are done in splitDocument method). \n",
    "    # It uses a heuristic method.\n",
    "    # show_topics: Show the scores across the topics\n",
    "    # plot_distributions: Plot the distributions\n",
    "    def predict_heuristic(self, document, show_topics=True, plot_distributions=True):\n",
    "        toWords = self.splitDocument(document)\n",
    "        \n",
    "        print(\"tokens:\")\n",
    "        print(toWords)\n",
    "        \n",
    "        scores_per_topic = np.zeros(self.no_topics)\n",
    "        \n",
    "        for word in toWords:\n",
    "            if word in self.wordsToIdx:\n",
    "                idx_word = self.wordsToIdx[word]\n",
    "                scores_per_topic += self.phi_extracted[:, idx_word]\n",
    "         \n",
    "        # normalize the probabilities\n",
    "        scores_per_topic = scores_per_topic / np.sum(scores_per_topic)\n",
    "        \n",
    "        if show_topics:\n",
    "            print(\"Topic scores (heuristical approach): \")\n",
    "            for i in range(len(scores_per_topic)):\n",
    "                print(\"Topic_{} score: {}\".format(i, scores_per_topic[i]))\n",
    "            print()\n",
    "        \n",
    "        if plot_distributions:\n",
    "            self.plot_topics_for_distribution(scores_per_topic)\n",
    "        \n",
    "        return scores_per_topic\n",
    "    \n",
    "    # the same as predict_heuristics, but uses a statistical approach (the formulas are summarized below in Task2-Q2)\n",
    "    # Instead of using the product of the probabilities, a log method is favoured (for numerical stability)  \n",
    "    def predict_statistics(self, document, show_topics=True, plot_distributions=True):\n",
    "        toWords = self.splitDocument(document)\n",
    "        \n",
    "        print(\"tokens:\")\n",
    "        print(toWords)\n",
    "        \n",
    "        topic_probability = np.sum(self.theta_extracted, axis=0) / self.no_documents\n",
    "        \n",
    "        assert(len(topic_probability) == self.no_topics)\n",
    "        assert(np.sum(topic_probability) >= 0.9999 and np.sum(topic_probability) <= 1.00001)\n",
    "        \n",
    "        # for numerical stability\n",
    "        epsilon = 0.0001\n",
    "        scores_per_topic = np.zeros(self.no_topics)\n",
    "        \n",
    "        for i in range(self.no_topics):\n",
    "            aux = math.log(topic_probability[i] + epsilon)\n",
    "            \n",
    "            for word in toWords:\n",
    "                if word in self.wordsToIdx:\n",
    "                    idx = self.wordsToIdx[word]\n",
    "                    aux = aux + math.log(self.phi_extracted[i][idx] + epsilon)\n",
    "            \n",
    "            scores_per_topic[i] = aux\n",
    "        \n",
    "        # normalize the probabilities\n",
    "        scores_per_topic = softmax(scores_per_topic)\n",
    "        \n",
    "        if show_topics:\n",
    "            print(\"Topic scores (statistical approach) per topics:\\n\")\n",
    "            for i in range(len(scores_per_topic)):\n",
    "                print(\"Topic_{} score: {}\".format(i, scores_per_topic[i]))\n",
    "            print()\n",
    "        \n",
    "        if plot_distributions:\n",
    "            self.plot_topics_for_distribution(scores_per_topic)\n",
    "        \n",
    "        return scores_per_topic\n",
    "    \n",
    "    # compute similarity between two documents. It get the scores_per_topic from the two predict methods for each document\n",
    "    # and computes KL Divergence (asymmetrical) or Jensen-Shannon Divergence (symmetrical).\n",
    "    # lower means that the documents are similar while higher means the documents are different\n",
    "    # show_similarity: Show the similarity between documents.\n",
    "    def similarity(self, topic_distribution_doc_1, topic_distribution_doc_2, type_algorithm=\"Jensen-Shannon\", show_similarity=True):\n",
    "        assert(len(topic_distribution_doc_1) == len(topic_distribution_doc_2))\n",
    "        \n",
    "        similarity_score = 0.0\n",
    "        epsilon = 0.00001\n",
    "        \n",
    "        # KL method\n",
    "        if type_algorithm == \"KL\":\n",
    "            for i in range(len(topic_distribution_doc_1)):\n",
    "                p = topic_distribution_doc_1[i] + epsilon\n",
    "                q = topic_distribution_doc_2[i] + epsilon\n",
    "                similarity_score += p * math.log(p / q)\n",
    "       \n",
    "        # Jensen-Shannon method\n",
    "        elif type_algorithm == \"Jensen-Shannon\":\n",
    "            for i in range(len(topic_distribution_doc_1)):\n",
    "                p = topic_distribution_doc_1[i] + epsilon\n",
    "                q = topic_distribution_doc_2[i] + epsilon\n",
    "                m = (p + q) / 2\n",
    "\n",
    "                similarity_score += p * math.log(p/m)\n",
    "                similarity_score += q * math.log(q/m)\n",
    "            \n",
    "            similarity_score = similarity_score / 2\n",
    "        \n",
    "        if show_similarity:\n",
    "            print(\"Similarity score is {} (using {})\".format(similarity_score, type_algorithm))\n",
    "            print()\n",
    "        \n",
    "        return similarity_score\n",
    "    \n",
    "    # plot a distribution given by distribution parameter (list of probabilities)\n",
    "    def plot_topics_for_distribution(self, distribution):\n",
    "        fig, ax = plt.subplots(figsize=(5,5));\n",
    "        patches = ax.bar(np.arange(len(distribution)), distribution)\n",
    "        ax.set_xlabel('Topic index', fontsize=10)\n",
    "        ax.set_ylabel('Score per topic', fontsize=10)\n",
    "        ax.set_xticks(np.linspace(0,self.no_topics - 1, self.no_topics))\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "                \n",
    "    # show theta_list at iteration (traced by mcmc)\n",
    "    # default iteration = last\n",
    "    def show_theta(self, iteration=-1):\n",
    "        print(\"Show theta at iter no {}:\\n\".format(iteration)) \n",
    "        for i in range(self.no_documents):\n",
    "            print(self.mcmc.trace(\"ctheta_{}\".format(i))[iteration])\n",
    "        print()\n",
    "    \n",
    "    # show phi_list at iteration (traced by mcmc)\n",
    "    # default iteration = last\n",
    "    def show_phi(self, iteration=-1):\n",
    "        print(\"Show phi at iter no {}:\\n\".format(iteration))\n",
    "        for i in range(self.no_topics):  \n",
    "            print(self.mcmc.trace(\"cphi_{}\".format(i))[iteration])\n",
    "        print()\n",
    "    \n",
    "    # show z_list at iteration (traced by mcmc)\n",
    "    # default iteration = last\n",
    "    def show_z(self, iteration=-1):\n",
    "        print(\"Show z at iter no {}:\\n\".format(iteration))\n",
    "        for i in range(self.no_documents):\n",
    "            aux = []\n",
    "            for j in range(self.doc_lengths_list[i]):\n",
    "                aux.append(self.mcmc.trace(\"z_{}-{}\".format(i, j))[iteration])\n",
    "            print(aux)\n",
    "\n",
    "    # split document into tokens while applying some cleaning techniques\n",
    "    def splitDocument(self, document):\n",
    "        special_characters = set(punctuation)\n",
    "        \n",
    "        # to lower-case\n",
    "        res = document.lower()\n",
    "        # tokenize\n",
    "        res = nltk.word_tokenize(res)\n",
    "        # remove any words of length <= 1. The others are \"stemmed\".\n",
    "        res = [self.stemmer.stem(word) for word in res if len(word) > 1]\n",
    "        # remove stop-words\n",
    "        res = [word for word in res if word not in self.stop_words]\n",
    "        # remove words that contain non-characters (digits, special characters)\n",
    "        res = [word for word in res if word.isalpha()]\n",
    "    \n",
    "        return res\n",
    "    \n",
    "    # preprocess the corpus: convert to lower-case, then transform the documents \n",
    "    # to tokens while applying a stemming over them\n",
    "    # it returns the corpus containing the new tokens, the corpus containing indexes as words,\n",
    "    # the word to idx dictionary and the idx to word dictionary\n",
    "    def preprocess(self, corpus):\n",
    "        # convert document to tokens\n",
    "        toTokens_corpus = [self.splitDocument(document) for document in corpus]\n",
    "                \n",
    "        wordsToIdx, idxToWords = {}, {}\n",
    "        for tokens_list in toTokens_corpus:\n",
    "            for token in tokens_list:\n",
    "                if token not in wordsToIdx and token not in self.stop_words:\n",
    "                    wordsToIdx[token] = len(wordsToIdx)\n",
    "                    idxToWords[len(idxToWords)] = token\n",
    "        \n",
    "        toIdx_corpus = [[wordsToIdx[word] for word in tokens_list if word in wordsToIdx] for tokens_list in toTokens_corpus]\n",
    "        toTokens_corpus = [[idxToWords[idx] for idx in idx_list] for idx_list in toIdx_corpus]\n",
    "        return (toTokens_corpus, toIdx_corpus, wordsToIdx, idxToWords)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To tokens corpus (tokens):\n",
      "\n",
      "['aaa', 'bbb', 'aaa']\n",
      "\n",
      "['bbb', 'aaa', 'bbb']\n",
      "\n",
      "['aaa', 'bbb', 'bbb', 'aaa']\n",
      "\n",
      "['uuu', 'vvv']\n",
      "\n",
      "['uuu', 'vvv', 'vvv']\n",
      "\n",
      "['uuu', 'vvv', 'vvv', 'uuu']\n",
      "\n",
      "\n",
      "To idx corpus:\n",
      "\n",
      "[0, 1, 0]\n",
      "\n",
      "[1, 0, 1]\n",
      "\n",
      "[0, 1, 1, 0]\n",
      "\n",
      "[2, 3]\n",
      "\n",
      "[2, 3, 3]\n",
      "\n",
      "[2, 3, 3, 2]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/PP/lib/python3.6/site-packages/pymc/MCMC.py:81: UserWarning: Instantiating a Model object directly is deprecated. We recommend passing variables directly to the Model subclass.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 15000 of 15000 complete in 34.8 sec"
     ]
    }
   ],
   "source": [
    "SANITY_CHECK = [\"aaa bbb aaa\",\n",
    "                \"bbb aaa bbb\",\n",
    "                \"aaa bbb bbb aaa\",\n",
    "                \"uuu vvv\",\n",
    "                \"uuu vvv vvv\",\n",
    "                \"uuu vvv vvv uuu\"]\n",
    "\n",
    "lda_model = LatentDirichletAllocation(no_topics=2, initial_corpus=SANITY_CHECK, alpha=0.85, beta=0.85)\n",
    "lda_model.fit(no_iter=15000, burn=5000, burn_till_tuned=0, extraction_type=\"last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the representative words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representative words for each topic:\n",
      "topic_0 : ['aaa', 'bbb']\n",
      "\n",
      "topic_1 : ['uuu', 'vvv']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = lda_model.getRepresentativeWordsForTopics(show_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show $\\theta$ and $\\phi$ values (Check that they change their values during each iteration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show theta at iter no 1:\n",
      "\n",
      "[[0.87500957 0.12499043]]\n",
      "[[0.67310815 0.32689185]]\n",
      "[[0.72559936 0.27440064]]\n",
      "[[0.28099164 0.71900836]]\n",
      "[[0.01195168 0.98804832]]\n",
      "[[0.03925771 0.96074229]]\n",
      "\n",
      "Show theta at iter no -1:\n",
      "\n",
      "[[0.94851183 0.05148817]]\n",
      "[[0.89055955 0.10944045]]\n",
      "[[0.97124875 0.02875125]]\n",
      "[[0.39468128 0.60531872]]\n",
      "[[0.19713729 0.80286271]]\n",
      "[[0.62615497 0.37384503]]\n",
      "\n",
      "Show phi at iter no 1:\n",
      "\n",
      "[[0.34206988 0.46240134 0.05121142 0.14431736]]\n",
      "[[0.1085538  0.19151143 0.3510647  0.34887006]]\n",
      "\n",
      "Show phi at iter no -1:\n",
      "\n",
      "[[0.49344166 0.40993789 0.05098602 0.04563443]]\n",
      "[[0.14101139 0.13550352 0.26600115 0.45748393]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model.show_theta(iteration=1)\n",
    "lda_model.show_theta(iteration=-1)\n",
    "lda_model.show_phi(iteration=1)\n",
    "lda_model.show_phi(iteration=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the values $z_{i,j}$ (the topics assigned to each word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show z at iter no -1:\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[1, 1]\n",
      "[1, 1, 1]\n",
      "[1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "lda_model.show_z()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on the documents from the project's requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To tokens corpus (tokens):\n",
      "\n",
      "['peanut', 'butter', 'sandwich', 'breakfast']\n",
      "\n",
      "['like', 'eat', 'almond', 'peanut', 'walnut']\n",
      "\n",
      "['neighbor', 'got', 'littl', 'dog', 'yesterday']\n",
      "\n",
      "['cat', 'dog', 'mortal', 'enemi']\n",
      "\n",
      "['feed', 'peanut', 'dog']\n",
      "\n",
      "\n",
      "To idx corpus:\n",
      "\n",
      "[0, 1, 2, 3]\n",
      "\n",
      "[4, 5, 6, 0, 7]\n",
      "\n",
      "[8, 9, 10, 11, 12]\n",
      "\n",
      "[13, 11, 14, 15]\n",
      "\n",
      "[16, 0, 11]\n",
      "\n",
      " [-----------------100%-----------------] 25000 of 25000 complete in 62.3 sec"
     ]
    }
   ],
   "source": [
    "DOCUMENTS = [\"I had a peanuts butter sandwich's for breakfast.\",\n",
    "             \"I like to eat almonds, peanuts and walnuts.\",\n",
    "             \"My neighbor got a little dog yesterday.\",\n",
    "             \"Cats and dogs are mortal enemies.\",\n",
    "             \"You mustn’t feed peanuts to your dog.\"\n",
    "            ]\n",
    "\n",
    "lda_model = LatentDirichletAllocation(no_topics=2, initial_corpus=DOCUMENTS, alpha=0.9, beta=0.95)\n",
    "lda_model.fit(no_iter=25000, burn=5000, burn_till_tuned=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the representative words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representative words for each topic:\n",
      "topic_0 : ['peanut', 'sandwich', 'breakfast', 'like', 'eat', 'almond', 'walnut', 'feed']\n",
      "\n",
      "topic_1 : ['butter', 'neighbor', 'got', 'littl', 'dog', 'yesterday', 'cat', 'mortal', 'enemi']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = lda_model.getRepresentativeWordsForTopics(show_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show $\\theta$ and $\\phi$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta extracted: \n",
      " [[0.77281548 0.22718452]\n",
      " [0.60898039 0.39101961]\n",
      " [0.22890158 0.77109842]\n",
      " [0.17998569 0.82001431]\n",
      " [0.4607822  0.5392178 ]] \n",
      "\n",
      "Phi extracted: \n",
      " [[0.18319073 0.04239931 0.16481528 0.05899242 0.0365015  0.10333297\n",
      "  0.05095861 0.11904838 0.01730587 0.03406527 0.0287848  0.00850643\n",
      "  0.01953037 0.03252958 0.01998226 0.02135987 0.05869637]\n",
      " [0.02821443 0.0547213  0.0047958  0.02059613 0.03117222 0.04367728\n",
      "  0.02725902 0.08961512 0.04107016 0.06551374 0.05420533 0.09219335\n",
      "  0.03896217 0.09457609 0.07231096 0.19245195 0.04866497]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Theta extracted: \\n {} \\n\".format(lda_model.theta_extracted))\n",
    "print(\"Phi extracted: \\n {} \\n\".format(lda_model.phi_extracted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the values $z_{i,j}$ (the topics assigned to each word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show z at iter no -1:\n",
      "\n",
      "[0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "lda_model.show_z()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on real documents from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download several documents about world's capitals (London, Berlin, Bucharest, etc) and several documents about ML Techniques (Reinforcement learning, Supervised learning, Unsupervised learning, etc) from Wikipedia using DocumentExtractor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities1 = DocumentExtractor(\"London\").getContent()\n",
    "cities2 = DocumentExtractor(\"Berlin\").getContent()\n",
    "cities3 = DocumentExtractor(\"Bucharest\").getContent()\n",
    "cities4 = DocumentExtractor(\"Madrid\").getContent()\n",
    "cities5 = DocumentExtractor(\"Lisbon\").getContent()\n",
    "cities6 = DocumentExtractor(\"Rome\").getContent()\n",
    "cities7 = DocumentExtractor(\"Vienna\").getContent()\n",
    "cities8 = DocumentExtractor(\"Sofia\").getContent()\n",
    "\n",
    "\n",
    "science1 = DocumentExtractor(\"Reinforcement learning\").getContent()\n",
    "science2 = DocumentExtractor(\"Supervised learning\").getContent()\n",
    "science3 = DocumentExtractor(\"Unsupervised learning\").getContent()\n",
    "science4 = DocumentExtractor(\"Autoencoder\").getContent()\n",
    "science5 = DocumentExtractor(\"k-means clustering\").getContent()\n",
    "science6 = DocumentExtractor(\"Linear regression\").getContent()\n",
    "science7 = DocumentExtractor(\"Support vector machine\").getContent()\n",
    "science8 = DocumentExtractor(\"Logistic regression\").getContent()\n",
    "\n",
    "\n",
    "DOCUMENTS = [cities1, cities2, cities3, cities4, cities5, cities6, cities7, cities8, science1, science2, science3, science4, science5, science6, science7, science8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To tokens corpus (tokens):\n",
      "\n",
      "['london', 'capit', 'largest', 'citi', 'england', 'unit', 'kingdom', 'citi', 'stand', 'river', 'thame', 'england', 'head', 'km', 'estuari', 'lead', 'north', 'sea', 'london', 'major', 'settlement', 'two', 'millennia', 'londinium', 'found', 'roman', 'citi', 'london', 'london', 'ancient', 'core', 'financi', 'centr', 'area', 'squar', 'mile', 'colloqui', 'known', 'squar', 'mile', 'retain', 'boundari', 'close', 'follow', 'mediev', 'limit', 'adjac', 'citi', 'westminst', 'inner', 'london', 'borough', 'centuri', 'locat', 'much', 'nation', 'govern', 'thirti', 'one', 'addit', 'borough', 'north', 'south', 'river', 'also', 'compris', 'modern', 'london', 'london', 'govern', 'mayor', 'london', 'london', 'one', 'world', 'import', 'global', 'citi', 'call', 'world', 'power', 'desir', 'influenti', 'visit', 'expens', 'sustain', 'citi', 'exert', 'consider', 'impact', 'upon', 'art', 'commerc', 'educ', 'entertain', 'fashion', 'financ', 'healthcar', 'media', 'profession', 'servic', 'research', 'develop', 'tourism', 'transport', 'london', 'rank', 'major', 'citi', 'econom', 'perform', 'one', 'largest', 'financi', 'centr', 'either', 'metropolitan', 'area', 'gdp', 'citi', 'measur', 'intern', 'arriv', 'busiest', 'citi', 'airport', 'system', 'measur', 'passeng', 'traffic', 'lead', 'invest', 'destin', 'host', 'intern', 'retail', 'ani', 'citi', 'london', 'number', 'billionair', 'ani', 'citi', 'europ', 'moscow', 'london', 'highest', 'number', 'ultra', 'individu', 'europ', 'london', 'univers', 'form', 'largest', 'concentr', 'higher', 'educ', 'institut', 'europ', 'london', 'home', 'high', 'rank', 'institut', 'imperi', 'colleg', 'london', 'natur', 'appli', 'scienc', 'london', 'school', 'econom', 'social', 'scienc', 'london', 'becam', 'first', 'citi', 'host', 'three', 'modern', 'summer', 'olymp', 'divers', 'rang', 'peopl', 'cultur', 'languag', 'spoken', 'region', 'estim', 'municip', 'popul', 'correspond', 'greater', 'london', 'popul', 'ani', 'citi', 'europ', 'account', 'uk', 'popul', 'london', 'urban', 'area', 'third', 'popul', 'europ', 'moscow', 'pari', 'inhabit', 'census', 'london', 'commut', 'belt', 'popul', 'europ', 'moscow', 'metropolitan', 'area', 'inhabit', 'london', 'contain', 'four', 'world', 'heritag', 'site', 'tower', 'london', 'kew', 'garden', 'site', 'compris', 'palac', 'westminst', 'westminst', 'abbey', 'st', 'margaret', 'church', 'histor', 'settlement', 'greenwich', 'royal', 'observatori', 'greenwich', 'defin', 'prime', 'meridian', 'longitud', 'greenwich', 'mean', 'time', 'landmark', 'includ', 'buckingham', 'palac', 'london', 'eye', 'piccadilli', 'circus', 'st', 'paul', 'cathedr', 'tower', 'bridg', 'trafalgar', 'squar', 'shard', 'london', 'numer', 'museum', 'galleri', 'librari', 'sport', 'event', 'includ', 'british', 'museum', 'nation', 'galleri', 'natur', 'histori', 'museum', 'tate', 'modern', 'british', 'librari', 'west', 'end', 'theatr', 'london', 'underground', 'oldest', 'underground', 'railway', 'network', 'world']\n",
      "\n",
      "['berlin', 'german', 'listen', 'capit', 'largest', 'citi', 'germani', 'area', 'popul', 'inhabit', 'decemb', 'make', 'citi', 'european', 'union', 'accord', 'popul', 'within', 'citi', 'limit', 'citi', 'also', 'one', 'germani', 'feder', 'state', 'surround', 'state', 'brandenburg', 'contigu', 'potsdam', 'brandenburg', 'capit', 'two', 'citi', 'center', 'capit', 'region', 'six', 'million', 'inhabit', 'area', 'germani', 'metropolitan', 'region', 'region', 'berlin', 'straddl', 'bank', 'river', 'spree', 'flow', 'river', 'havel', 'tributari', 'river', 'elb', 'western', 'borough', 'spandau', 'among', 'citi', 'main', 'topograph', 'featur', 'mani', 'lake', 'western', 'southeastern', 'borough', 'form', 'spree', 'havel', 'dahm', 'river', 'largest', 'lake', 'müggelse', 'due', 'locat', 'european', 'plain', 'berlin', 'influenc', 'temper', 'season', 'climat', 'citi', 'area', 'compos', 'forest', 'park', 'garden', 'river', 'canal', 'lake', 'citi', 'lie', 'central', 'german', 'dialect', 'area', 'berlin', 'dialect', 'variant', 'marchian', 'dialect', 'first', 'document', 'centuri', 'cross', 'two', 'import', 'histor', 'trade', 'rout', 'berlin', 'becam', 'capit', 'margravi', 'brandenburg', 'kingdom', 'prussia', 'german', 'empir', 'weimar', 'republ', 'third', 'reich', 'berlin', 'municip', 'world', 'world', 'war', 'ii', 'subsequ', 'occup', 'victori', 'countri', 'citi', 'divid', 'west', 'berlin', 'becam', 'de', 'facto', 'west', 'german', 'exclav', 'surround', 'berlin', 'wall', 'east', 'german', 'territori', 'east', 'berlin', 'declar', 'capit', 'east', 'germani', 'bonn', 'becam', 'west', 'german', 'capit', 'follow', 'german', 'reunif', 'berlin', 'onc', 'becam', 'capit', 'germani', 'berlin', 'world', 'citi', 'cultur', 'polit', 'media', 'scienc', 'economi', 'base', 'firm', 'servic', 'sector', 'encompass', 'divers', 'rang', 'creativ', 'industri', 'research', 'facil', 'media', 'corpor', 'convent', 'venu', 'berlin', 'serv', 'continent', 'hub', 'air', 'rail', 'traffic', 'high', 'complex', 'public', 'transport', 'network', 'metropoli', 'popular', 'tourist', 'destin', 'signific', 'industri', 'also', 'includ', 'pharmaceut', 'biomed', 'engin', 'clean', 'tech', 'biotechnolog', 'construct', 'electron', 'berlin', 'home', 'univers', 'humboldt', 'universität', 'zu', 'berlin', 'hu', 'berlin', 'technisch', 'universität', 'berlin', 'tu', 'berlin', 'freie', 'universität', 'berlin', 'free', 'univers', 'berlin', 'universität', 'der', 'künste', 'univers', 'art', 'udk', 'esmt', 'berlin', 'berlin', 'school', 'econom', 'law', 'zoolog', 'garden', 'visit', 'zoo', 'europ', 'one', 'popular', 'worldwid', 'world', 'oldest', 'movi', 'studio', 'complex', 'berlin', 'increas', 'popular', 'locat', 'intern', 'film', 'product', 'citi', 'well', 'known', 'festiv', 'divers', 'architectur', 'nightlif', 'contemporari', 'art', 'veri', 'high', 'qualiti', 'live', 'sinc', 'berlin', 'seen', 'emerg', 'cosmopolitan', 'entrepreneuri', 'contain', 'three', 'world', 'heritag', 'site', 'museum', 'island', 'palac', 'park', 'potsdam', 'berlin', 'berlin', 'modern', 'hous', 'estat', 'landmark', 'includ', 'brandenburg', 'gate', 'reichstag', 'build', 'potsdam', 'platz', 'memori', 'murder', 'jew', 'europ', 'berlin', 'wall', 'memori', 'east', 'side', 'galleri', 'berlin', 'victori', 'column', 'berlin', 'cathedr', 'berlin', 'televis', 'tower', 'tallest', 'structur', 'germani', 'berlin', 'numer', 'museum', 'galleri', 'librari', 'orchestra', 'sport', 'event', 'includ', 'old', 'nation', 'galleri', 'bode', 'museum', 'pergamon', 'museum', 'german', 'histor', 'museum', 'jewish', 'museum', 'berlin', 'natur', 'histori', 'museum', 'humboldt', 'forum', 'schedul', 'open', 'late', 'berlin', 'state', 'librari', 'berlin', 'state', 'opera', 'berlin', 'philharmon', 'berlin', 'marathon']\n",
      "\n",
      "['bucharest', 'uk', 'us', 'romanian', 'bucurești', 'bukuˈreʃtʲ', 'listen', 'capit', 'largest', 'citi', 'romania', 'well', 'cultur', 'industri', 'financi', 'centr', 'southeast', 'countri', 'bank', 'dâmbovița', 'river', 'less', 'km', 'mi', 'north', 'danub', 'river', 'bulgarian', 'border', 'bucharest', 'first', 'mention', 'document', 'becam', 'capit', 'romania', 'centr', 'romanian', 'media', 'cultur', 'art', 'architectur', 'mix', 'histor', 'neoclass', 'art', 'nouveau', 'interbellum', 'bauhaus', 'art', 'deco', 'communist', 'era', 'modern', 'period', 'two', 'world', 'war', 'citi', 'eleg', 'architectur', 'sophist', 'elit', 'earn', 'bucharest', 'nicknam', 'pari', 'east', 'romanian', 'parisul', 'estului', 'littl', 'pari', 'romanian', 'micul', 'pari', 'although', 'build', 'district', 'histor', 'citi', 'centr', 'heavili', 'damag', 'destroy', 'war', 'earthquak', 'even', 'nicola', 'ceaușescu', 'program', 'systemat', 'mani', 'surviv', 'renov', 'recent', 'year', 'citi', 'experienc', 'econom', 'cultur', 'boom', 'one', 'citi', 'europ', 'accord', 'financi', 'time', 'cbre', 'techcrunch', 'uipath', 'global', 'startup', 'found', 'bucharest', 'reach', 'billion', 'valuat', 'sinc', 'bucharest', 'host', 'largest', 'high', 'tech', 'summit', 'southeast', 'europ', 'romania', 'blockchain', 'summit', 'histor', 'citi', 'centr', 'list', 'endang', 'world', 'monument', 'watch', 'bucharest', 'european', 'citi', 'highest', 'growth', 'tourist', 'stay', 'night', 'accord', 'mastercard', 'global', 'index', 'urban', 'destin', 'past', 'two', 'consecut', 'year', 'bucharest', 'rank', 'european', 'destin', 'highest', 'potenti', 'develop', 'accord', 'census', 'inhabit', 'live', 'within', 'citi', 'limit', 'decreas', 'census', 'ad', 'satellit', 'town', 'around', 'urban', 'area', 'propos', 'metropolitan', 'area', 'bucharest', 'would', 'popul', 'million', 'peopl', 'dure', 'pandem', 'romanian', 'govern', 'use', 'million', 'peopl', 'basi', 'report', 'infect', 'rate', 'citi', 'bucharest', 'fourth', 'largest', 'citi', 'european', 'union', 'popul', 'within', 'citi', 'limit', 'berlin', 'madrid', 'rome', 'econom', 'bucharest', 'prosper', 'citi', 'romania', 'citi', 'number', 'larg', 'convent', 'facil', 'educ', 'institut', 'cultur', 'venu', 'tradit', 'shop', 'arcad', 'recreat', 'area', 'citi', 'proper', 'administr', 'known', 'municip', 'bucharest', 'municipiul', 'bucurești', 'administr', 'level', 'nation', 'counti', 'subdivid', 'six', 'sector', 'govern', 'local', 'mayor']\n",
      "\n",
      "['madrid', 'spanish', 'maˈðɾið', 'capit', 'citi', 'spain', 'citi', 'almost', 'million', 'inhabit', 'metropolitan', 'area', 'popul', 'approxim', 'million', 'citi', 'european', 'union', 'eu', 'surpass', 'onli', 'berlin', 'monocentr', 'metropolitan', 'area', 'eu', 'surpass', 'onli', 'pari', 'municip', 'cover', 'sq', 'mi', 'lie', 'river', 'manzanar', 'center', 'countri', 'communiti', 'madrid', 'region', 'also', 'capit', 'capit', 'citi', 'spain', 'seat', 'govern', 'resid', 'spanish', 'monarch', 'madrid', 'also', 'polit', 'econom', 'cultur', 'centr', 'countri', 'mayor', 'josé', 'lui', 'peopl', 'parti', 'madrid', 'urban', 'agglomer', 'gdp', 'european', 'union', 'influenc', 'polit', 'educ', 'entertain', 'environ', 'media', 'fashion', 'scienc', 'cultur', 'art', 'contribut', 'status', 'one', 'world', 'major', 'global', 'citi', 'madrid', 'home', 'two', 'footbal', 'club', 'real', 'madrid', 'atlético', 'madrid', 'due', 'econom', 'output', 'high', 'standard', 'live', 'market', 'size', 'madrid', 'consid', 'major', 'financi', 'centr', 'lead', 'econom', 'hub', 'iberian', 'peninsula', 'southern', 'europ', 'host', 'head', 'offic', 'vast', 'major', 'major', 'spanish', 'compani', 'telefónica', 'iag', 'repsol', 'madrid', 'also', 'liveabl', 'citi', 'world', 'accord', 'monocl', 'magazin', 'hous', 'headquart', 'un', 'world', 'tourism', 'organ', 'unwto', 'general', 'secretariat', 'segib', 'organ', 'state', 'oei', 'public', 'interest', 'oversight', 'board', 'piob', 'also', 'host', 'major', 'intern', 'regul', 'promot', 'spanish', 'languag', 'stand', 'committe', 'associ', 'spanish', 'languag', 'academi', 'headquart', 'royal', 'spanish', 'academi', 'rae', 'instituto', 'cervant', 'foundat', 'urgent', 'spanish', 'fundéu', 'bbva', 'madrid', 'organis', 'fair', 'fitur', 'arco', 'simo', 'tci', 'madrid', 'fashion', 'madrid', 'possess', 'modern', 'infrastructur', 'preserv', 'look', 'feel', 'mani', 'histor', 'neighbourhood', 'street', 'landmark', 'includ', 'plaza', 'mayor', 'royal', 'palac', 'madrid', 'royal', 'theatr', 'restor', 'opera', 'hous', 'buen', 'retiro', 'park', 'found', 'nation', 'librari', 'build', 'found', 'contain', 'spain', 'histor', 'archiv', 'mani', 'nation', 'museum', 'golden', 'triangl', 'art', 'locat', 'along', 'paseo', 'del', 'prado', 'compris', 'three', 'art', 'museum', 'prado', 'museum', 'reina', 'sofía', 'museum', 'museum', 'modern', 'art', 'museum', 'complement', 'hold', 'two', 'museum', 'cibel', 'palac', 'fountain', 'becom', 'one', 'monument', 'symbol', 'citi']\n",
      "\n",
      "['lisbon', 'portugues', 'lisboa', 'liʒˈboɐ', 'listen', 'capit', 'largest', 'citi', 'portug', 'estim', 'popul', 'within', 'administr', 'limit', 'area', 'lisbon', 'urban', 'area', 'extend', 'beyond', 'citi', 'administr', 'limit', 'popul', 'around', 'million', 'peopl', 'popul', 'urban', 'area', 'european', 'union', 'million', 'peopl', 'live', 'lisbon', 'metropolitan', 'area', 'repres', 'approxim', 'countri', 'popul', 'mainland', 'europ', 'westernmost', 'capit', 'citi', 'onli', 'one', 'along', 'atlant', 'coast', 'lisbon', 'lie', 'western', 'iberian', 'peninsula', 'atlant', 'ocean', 'river', 'tagus', 'westernmost', 'portion', 'metro', 'area', 'portugues', 'riviera', 'form', 'westernmost', 'point', 'continent', 'europ', 'culmin', 'cabo', 'da', 'roca', 'lisbon', 'recognis', 'global', 'citi', 'becaus', 'import', 'financ', 'commerc', 'media', 'entertain', 'art', 'intern', 'trade', 'educ', 'tourism', 'lisbon', 'one', 'two', 'portugues', 'citi', 'alongsid', 'porto', 'recognis', 'global', 'citi', 'one', 'major', 'econom', 'centr', 'contin', 'grow', 'financi', 'sector', 'one', 'largest', 'contain', 'port', 'europ', 'atlant', 'coast', 'addit', 'humberto', 'delgado', 'airport', 'serv', 'million', 'passeng', 'busiest', 'airport', 'portug', 'busiest', 'iberian', 'peninsula', 'busiest', 'europ', 'motorway', 'network', 'rail', 'system', 'alfa', 'pendular', 'link', 'main', 'citi', 'portug', 'lisbon', 'citi', 'citi', 'southern', 'europ', 'istanbul', 'rome', 'barcelona', 'milan', 'athen', 'venic', 'madrid', 'florenc', 'tourist', 'lisbon', 'region', 'higher', 'gdp', 'ppp', 'per', 'capita', 'ani', 'region', 'portug', 'gdp', 'amount', 'us', 'billion', 'thus', 'per', 'capita', 'citi', 'occupi', 'place', 'highest', 'gross', 'earn', 'world', 'headquart', 'multin', 'corpor', 'portug', 'locat', 'lisbon', 'area', 'also', 'polit', 'centr', 'countri', 'seat', 'govern', 'resid', 'head', 'state', 'lisbon', 'one', 'oldest', 'citi', 'world', 'european', 'capit', 'citi', 'athen', 'predat', 'modern', 'european', 'capit', 'centuri', 'julius', 'caesar', 'made', 'municipium', 'call', 'felicita', 'julia', 'ad', 'name', 'olissipo', 'rule', 'seri', 'german', 'tribe', 'centuri', 'captur', 'moor', 'centuri', 'crusad', 'afonso', 'henriqu', 'reconqu', 'citi', 'sinc', 'polit', 'econom', 'cultur', 'center', 'portug']\n",
      "\n",
      "['road', 'thoroughfar', 'rout', 'way', 'land', 'two', 'place', 'pave', 'otherwis', 'improv', 'allow', 'travel', 'foot', 'form', 'convey', 'includ', 'motor', 'vehicl', 'cart', 'bicycl', 'hors', 'road', 'consist', 'one', 'two', 'roadway', 'british', 'english', 'carriageway', 'one', 'lane', 'ani', 'associ', 'sidewalk', 'british', 'english', 'pavement', 'road', 'verg', 'bike', 'path', 'road', 'use', 'may', 'may', 'parallel', 'road', 'name', 'road', 'includ', 'parkway', 'avenu', 'freeway', 'motorway', 'expressway', 'tollway', 'interst', 'highway', 'thoroughfar', 'primari', 'secondari', 'tertiari', 'local', 'road']\n",
      "\n",
      "['vietnam', 'vietnames', 'việt', 'nam', 'vîət', 'nāːm', 'listen', 'offici', 'socialist', 'republ', 'vietnam', 'vietnames', 'cộng', 'hòa', 'xã', 'hội', 'chủ', 'nghĩa', 'việt', 'nam', 'countri', 'southeast', 'asia', 'easternmost', 'countri', 'indochines', 'peninsula', 'estim', 'million', 'inhabit', 'popul', 'countri', 'world', 'vietnam', 'share', 'land', 'border', 'china', 'north', 'lao', 'cambodia', 'west', 'share', 'maritim', 'border', 'thailand', 'gulf', 'thailand', 'philippin', 'indonesia', 'malaysia', 'south', 'china', 'sea', 'capit', 'citi', 'hanoi', 'popul', 'citi', 'ho', 'chi', 'minh', 'citi', 'also', 'known', 'former', 'name', 'saigon', 'archaeolog', 'excav', 'indic', 'vietnam', 'inhabit', 'earli', 'paleolith', 'age', 'ancient', 'vietnames', 'nation', 'center', 'red', 'river', 'valley', 'nearbi', 'coastal', 'area', 'annex', 'han', 'dynasti', 'centuri', 'bc', 'subsequ', 'made', 'vietnam', 'divis', 'imperi', 'china', 'millennium', 'first', 'independ', 'monarchi', 'emerg', 'centuri', 'ad', 'pave', 'way', 'success', 'imperi', 'dynasti', 'nation', 'expand', 'southward', 'indochina', 'peninsula', 'colonis', 'french', 'late', 'centuri', 'modern', 'vietnam', 'born', 'upon', 'proclam', 'independ', 'franc', 'follow', 'vietnames', 'victori', 'french', 'first', 'indochina', 'war', 'end', 'nation', 'divid', 'two', 'rival', 'state', 'communist', 'north', 'south', 'conflict', 'intensifi', 'vietnam', 'war', 'saw', 'extens', 'us', 'intervent', 'support', 'south', 'vietnam', 'end', 'north', 'vietnames', 'victori', 'north', 'south', 'vietnam', 'reunifi', 'communist', 'state', 'unitari', 'socialist', 'govern', 'countri', 'becam', 'econom', 'polit', 'isol', 'communist', 'parti', 'initi', 'seri', 'econom', 'polit', 'reform', 'facilit', 'vietnames', 'integr', 'world', 'polit', 'global', 'economi', 'result', 'success', 'reform', 'vietnam', 'enjoy', 'high', 'gdp', 'growth', 'rate', 'consist', 'rank', 'among', 'countri', 'world', 'nevertheless', 'face', 'challeng', 'includ', 'corrupt', 'pollut', 'poverti', 'inadequ', 'social', 'welfar', 'poor', 'human', 'right', 'record', 'includ', 'increas', 'persecut', 'religi', 'group', 'human', 'right', 'advoc', 'intensifi', 'restrict', 'civil', 'liberti', 'vietnam', 'establish', 'diplomat', 'relat', 'countri', 'member', 'intern', 'organis', 'unit', 'nation', 'un', 'associ', 'southeast', 'asian', 'nation', 'asean', 'econom', 'cooper', 'apec', 'forum', 'world', 'trade', 'organ', 'wto']\n",
      "\n",
      "['sofia', 'bulgarian', 'софия', 'roman', 'sofiya', 'ipa', 'ˈsɔfijɐ', 'listen', 'capit', 'largest', 'citi', 'bulgaria', 'situat', 'eponym', 'valley', 'foot', 'vitosha', 'mountain', 'western', 'part', 'countri', 'citi', 'built', 'west', 'iskar', 'river', 'mani', 'miner', 'spring', 'sofia', 'central', 'miner', 'bath', 'humid', 'continent', 'climat', 'centr', 'balkan', 'midway', 'black', 'sea', 'adriat', 'sea', 'closest', 'aegean', 'serdica', 'antiqu', 'sredet', 'middl', 'age', 'sofia', 'area', 'human', 'habit', 'sinc', 'least', 'bc', 'record', 'histori', 'citi', 'begin', 'attest', 'conquest', 'serdica', 'roman', 'republ', 'bc', 'celtic', 'tribe', 'serdi', 'dure', 'declin', 'roman', 'empir', 'citi', 'raid', 'hun', 'visigoth', 'avar', 'slav', 'serdica', 'incorpor', 'bulgarian', 'empir', 'khan', 'krum', 'becam', 'known', 'sredet', 'byzantin', 'end', 'bulgarian', 'rule', 'reincorpor', 'reborn', 'bulgarian', 'empir', 'sredet', 'becam', 'major', 'administr', 'econom', 'cultur', 'literari', 'hub', 'conquest', 'ottoman', 'sofia', 'region', 'capit', 'rumelia', 'eyalet', 'ottoman', 'empir', 'key', 'provinc', 'europ', 'bulgarian', 'rule', 'restor', 'sofia', 'select', 'capit', 'third', 'bulgarian', 'state', 'next', 'year', 'usher', 'period', 'intens', 'demograph', 'econom', 'growth', 'sofia', 'largest', 'citi', 'european', 'union', 'surround', 'mountainsid', 'vitosha', 'southern', 'side', 'lyulin', 'western', 'side', 'balkan', 'mountain', 'north', 'make', 'third', 'highest', 'european', 'capit', 'andorra', 'la', 'vella', 'madrid', 'bulgaria', 'primat', 'citi', 'sofia', 'home', 'mani', 'major', 'local', 'univers', 'cultur', 'institut', 'commerci', 'compani', 'citi', 'describ', 'triangl', 'religi', 'toler', 'due', 'fact', 'three', 'templ', 'three', 'major', 'world', 'islam', 'situat', 'within', 'one', 'squar', 'sveta', 'nedelya', 'church', 'banya', 'bashi', 'mosqu', 'sofia', 'synagogu', 'triangl', 'recent', 'expand', 'squar', 'includ', 'cathol', 'cathedr', 'st', 'joseph', 'name', 'one', 'top', 'ten', 'best', 'place', 'busi', 'world', 'especi', 'inform', 'technolog', 'sofia', 'europ', 'afford', 'capit', 'visit', 'boyana', 'church', 'sofia', 'includ', 'onto', 'world', 'heritag', 'list', 'deconstruct', 'second', 'bulgarian', 'empir', 'hold', 'much', 'patrimoni', 'symbol', 'bulgarian', 'orthodox', 'church', 'cultur', 'signific', 'southeast', 'europ', 'sofia', 'home', 'nation', 'opera', 'ballet', 'bulgaria', 'nation', 'palac', 'cultur', 'vasil', 'levski', 'nation', 'stadium', 'ivan', 'vazov', 'nation', 'theatr', 'nation', 'archaeolog', 'museum', 'serdica', 'amphitheatr', 'museum', 'socialist', 'art', 'includ', 'mani', 'sculptur', 'poster', 'educ', 'visitor', 'lifestyl', 'communist', 'popul', 'sofia', 'declin', 'late', 'centuri', 'began', 'increas', 'sofia', 'host', 'million', 'resid', 'within', 'territori', 'concentr', 'countri', 'popul', 'within', 'percentil', 'countri', 'territori', 'urban', 'area', 'sofia', 'host', 'million', 'resid', 'within', 'compris', 'sofia', 'citi', 'provinc', 'part', 'sofia', 'provinc', 'dragoman', 'slivnitsa', 'kostinbrod', 'bozhurisht', 'svoge', 'elin', 'pelin', 'gorna', 'malina', 'ihtiman', 'kostenet', 'pernik', 'provinc', 'pernik', 'radomir', 'repres', 'countri', 'territori', 'metropolitan', 'area', 'sofia', 'base', 'upon', 'one', 'hour', 'car', 'travel', 'time', 'stretch', 'intern', 'includ', 'dimitrovgrad', 'serbia', 'metropolitan', 'region', 'sofia', 'inhabit', 'popul', 'million']\n",
      "\n",
      "['reinforc', 'learn', 'rl', 'area', 'machin', 'learn', 'concern', 'softwar', 'agent', 'ought', 'take', 'action', 'environ', 'order', 'maxim', 'notion', 'cumul', 'reward', 'reinforc', 'learn', 'one', 'three', 'basic', 'machin', 'learn', 'paradigm', 'alongsid', 'supervis', 'learn', 'unsupervis', 'learn', 'reinforc', 'learn', 'differ', 'supervis', 'learn', 'need', 'label', 'pair', 'present', 'need', 'action', 'explicit', 'correct', 'instead', 'focus', 'find', 'balanc', 'explor', 'unchart', 'territori', 'exploit', 'current', 'knowledg', 'environ', 'typic', 'state', 'form', 'markov', 'decis', 'process', 'mdp', 'becaus', 'mani', 'reinforc', 'learn', 'algorithm', 'context', 'use', 'dynam', 'program', 'techniqu', 'main', 'differ', 'classic', 'dynam', 'program', 'method', 'reinforc', 'learn', 'algorithm', 'latter', 'assum', 'knowledg', 'exact', 'mathemat', 'model', 'mdp', 'target', 'larg', 'mdps', 'exact', 'method', 'becom', 'infeas']\n",
      "\n",
      "['supervis', 'learn', 'machin', 'learn', 'task', 'learn', 'function', 'map', 'input', 'output', 'base', 'exampl', 'pair', 'infer', 'function', 'label', 'train', 'data', 'consist', 'set', 'train', 'exampl', 'supervis', 'learn', 'exampl', 'pair', 'consist', 'input', 'object', 'typic', 'vector', 'desir', 'output', 'valu', 'also', 'call', 'supervisori', 'signal', 'supervis', 'learn', 'algorithm', 'analyz', 'train', 'data', 'produc', 'infer', 'function', 'use', 'map', 'new', 'exampl', 'optim', 'scenario', 'allow', 'algorithm', 'correct', 'determin', 'class', 'label', 'unseen', 'instanc', 'requir', 'learn', 'algorithm', 'general', 'train', 'data', 'unseen', 'situat', 'reason', 'way', 'see', 'induct', 'bias', 'parallel', 'task', 'human', 'anim', 'psycholog', 'often', 'refer', 'concept', 'learn']\n",
      "\n",
      "['unsupervis', 'learn', 'type', 'machin', 'learn', 'look', 'previous', 'undetect', 'pattern', 'data', 'set', 'label', 'minimum', 'human', 'supervis', 'contrast', 'supervis', 'learn', 'usual', 'make', 'use', 'data', 'unsupervis', 'learn', 'also', 'known', 'allow', 'model', 'probabl', 'densiti', 'input', 'form', 'one', 'three', 'main', 'categori', 'machin', 'learn', 'along', 'supervis', 'reinforc', 'learn', 'learn', 'relat', 'variant', 'make', 'use', 'supervis', 'unsupervis', 'techniqu', 'two', 'main', 'method', 'use', 'unsupervis', 'learn', 'princip', 'compon', 'cluster', 'analysi', 'cluster', 'analysi', 'use', 'unsupervis', 'learn', 'group', 'segment', 'dataset', 'share', 'attribut', 'order', 'extrapol', 'algorithm', 'relationship', 'cluster', 'analysi', 'branch', 'machin', 'learn', 'group', 'data', 'label', 'classifi', 'categor', 'instead', 'respond', 'feedback', 'cluster', 'analysi', 'identifi', 'common', 'data', 'react', 'base', 'presenc', 'absenc', 'common', 'new', 'piec', 'data', 'approach', 'help', 'detect', 'anomal', 'data', 'point', 'fit', 'either', 'group', 'onli', 'requir', 'call', 'unsupervis', 'learn', 'strategi', 'learn', 'new', 'featur', 'space', 'captur', 'characterist', 'origin', 'space', 'maxim', 'object', 'function', 'minimis', 'loss', 'function', 'therefor', 'generat', 'covari', 'matrix', 'unsupervis', 'learn', 'take', 'eigenvector', 'covari', 'matrix', 'becaus', 'linear', 'algebra', 'eigendecomposit', 'oper', 'maxim', 'varianc', 'known', 'princip', 'compon', 'analysi', 'similar', 'take', 'dataset', 'unsupervis', 'learn', 'pass', 'input', 'data', 'multipl', 'sigmoid', 'function', 'minimis', 'distanc', 'function', 'generat', 'result', 'data', 'known', 'autoencod', 'central', 'applic', 'unsupervis', 'learn', 'field', 'densiti', 'estim', 'statist', 'though', 'unsupervis', 'learn', 'encompass', 'mani', 'domain', 'involv', 'summar', 'explain', 'data', 'featur', 'could', 'contrast', 'supervis', 'learn', 'say', 'wherea', 'supervis', 'learn', 'intend', 'infer', 'condit', 'probabl', 'distribut', 'condit', 'label', 'input', 'data', 'unsupervis', 'learn', 'intend', 'infer', 'priori', 'probabl', 'distribut', 'generat', 'adversari', 'network', 'also', 'use', 'supervis', 'learn', 'though', 'also', 'appli', 'unsupervis', 'reinforc', 'techniqu']\n",
      "\n",
      "['autoencod', 'type', 'artifici', 'neural', 'network', 'use', 'learn', 'effici', 'data', 'code', 'unsupervis', 'manner', 'aim', 'autoencod', 'learn', 'represent', 'encod', 'set', 'data', 'typic', 'dimension', 'reduct', 'train', 'network', 'ignor', 'signal', 'nois', 'along', 'reduct', 'side', 'reconstruct', 'side', 'learnt', 'autoencod', 'tri', 'generat', 'reduc', 'encod', 'represent', 'close', 'possibl', 'origin', 'input', 'henc', 'name', 'sever', 'variant', 'exist', 'basic', 'model', 'aim', 'forc', 'learn', 'represent', 'input', 'assum', 'use', 'properti', 'exampl', 'regular', 'autoencod', 'spars', 'denois', 'contract', 'autoencod', 'proven', 'effect', 'learn', 'represent', 'subsequ', 'classif', 'task', 'variat', 'autoencod', 'recent', 'applic', 'generat', 'model', 'autoencod', 'effect', 'use', 'solv', 'mani', 'appli', 'problem', 'face', 'recognit', 'acquir', 'semant', 'mean', 'word']\n",
      "\n",
      "['cluster', 'method', 'vector', 'quantize', 'origin', 'signal', 'process', 'aim', 'partit', 'observ', 'cluster', 'observ', 'belong', 'cluster', 'nearest', 'mean', 'cluster', 'center', 'cluster', 'centroid', 'serv', 'prototyp', 'cluster', 'result', 'partit', 'data', 'space', 'voronoi', 'cell', 'cluster', 'minim', 'varianc', 'squar', 'euclidean', 'distanc', 'regular', 'euclidean', 'distanc', 'would', 'difficult', 'weber', 'problem', 'mean', 'optim', 'squar', 'error', 'wherea', 'onli', 'geometr', 'median', 'minim', 'euclidean', 'distanc', 'instanc', 'better', 'euclidean', 'solut', 'found', 'use', 'problem', 'comput', 'difficult', 'howev', 'effici', 'heurist', 'algorithm', 'converg', 'quick', 'local', 'optimum', 'usual', 'similar', 'algorithm', 'mixtur', 'gaussian', 'distribut', 'via', 'iter', 'refin', 'approach', 'employ', 'gaussian', 'mixtur', 'model', 'use', 'cluster', 'center', 'model', 'data', 'howev', 'cluster', 'tend', 'find', 'cluster', 'compar', 'spatial', 'extent', 'mechan', 'allow', 'cluster', 'differ', 'shape', 'algorithm', 'loos', 'relationship', 'neighbor', 'classifi', 'popular', 'machin', 'learn', 'techniqu', 'classif', 'often', 'confus', 'due', 'name', 'appli', 'neighbor', 'classifi', 'cluster', 'center', 'obtain', 'classifi', 'new', 'data', 'exist', 'cluster', 'known', 'nearest', 'centroid', 'classifi', 'rocchio', 'algorithm']\n",
      "\n",
      "['statist', 'linear', 'regress', 'linear', 'approach', 'model', 'relationship', 'scalar', 'respons', 'one', 'explanatori', 'variabl', 'also', 'known', 'depend', 'independ', 'variabl', 'case', 'one', 'explanatori', 'variabl', 'call', 'simpl', 'linear', 'regress', 'one', 'process', 'call', 'multipl', 'linear', 'regress', 'term', 'distinct', 'multivari', 'linear', 'regress', 'multipl', 'correl', 'depend', 'variabl', 'predict', 'rather', 'singl', 'scalar', 'linear', 'regress', 'relationship', 'model', 'use', 'linear', 'predictor', 'function', 'whose', 'unknown', 'model', 'paramet', 'estim', 'data', 'model', 'call', 'linear', 'model', 'common', 'condit', 'mean', 'respons', 'given', 'valu', 'explanatori', 'variabl', 'predictor', 'assum', 'affin', 'function', 'valu', 'less', 'common', 'condit', 'median', 'quantil', 'use', 'like', 'form', 'regress', 'analysi', 'linear', 'regress', 'focus', 'condit', 'probabl', 'distribut', 'respons', 'given', 'valu', 'predictor', 'rather', 'joint', 'probabl', 'distribut', 'variabl', 'domain', 'multivari', 'analysi', 'linear', 'regress', 'first', 'type', 'regress', 'analysi', 'studi', 'rigor', 'use', 'extens', 'practic', 'applic', 'becaus', 'model', 'depend', 'linear', 'unknown', 'paramet', 'easier', 'fit', 'model', 'relat', 'paramet', 'becaus', 'statist', 'properti', 'result', 'estim', 'easier', 'determin', 'linear', 'regress', 'mani', 'practic', 'use', 'applic', 'fall', 'one', 'follow', 'two', 'broad', 'categori', 'goal', 'predict', 'forecast', 'error', 'reduct', 'linear', 'regress', 'use', 'fit', 'predict', 'model', 'observ', 'data', 'set', 'valu', 'respons', 'explanatori', 'variabl', 'develop', 'model', 'addit', 'valu', 'explanatori', 'variabl', 'collect', 'without', 'accompani', 'respons', 'valu', 'fit', 'model', 'use', 'make', 'predict', 'respons', 'goal', 'explain', 'variat', 'respons', 'variabl', 'attribut', 'variat', 'explanatori', 'variabl', 'linear', 'regress', 'analysi', 'appli', 'quantifi', 'strength', 'relationship', 'respons', 'explanatori', 'variabl', 'particular', 'determin', 'whether', 'explanatori', 'variabl', 'may', 'linear', 'relationship', 'respons', 'identifi', 'subset', 'explanatori', 'variabl', 'may', 'contain', 'redund', 'inform', 'regress', 'model', 'often', 'fit', 'use', 'least', 'squar', 'approach', 'may', 'also', 'fit', 'way', 'minim', 'lack', 'fit', 'norm', 'least', 'absolut', 'deviat', 'regress', 'minim', 'penal', 'version', 'least', 'squar', 'cost', 'function', 'ridg', 'regress', 'penalti', 'lasso', 'penalti', 'convers', 'least', 'squar', 'approach', 'use', 'fit', 'model', 'linear', 'model', 'thus', 'although', 'term', 'least', 'squar', 'linear', 'model', 'close', 'link', 'synonym']\n",
      "\n",
      "['machin', 'learn', 'machin', 'svms', 'also', 'network', 'supervis', 'learn', 'model', 'associ', 'learn', 'algorithm', 'analyz', 'data', 'use', 'classif', 'regress', 'analysi', 'develop', 'bell', 'laboratori', 'vapnik', 'colleagu', 'boser', 'et', 'guyon', 'et', 'vapnik', 'et', 'present', 'one', 'robust', 'predict', 'method', 'base', 'statist', 'learn', 'framework', 'vc', 'theori', 'propos', 'vapnik', 'chervonenki', 'vapnik', 'given', 'set', 'train', 'exampl', 'mark', 'belong', 'one', 'two', 'categori', 'svm', 'train', 'algorithm', 'build', 'model', 'assign', 'new', 'exampl', 'one', 'categori', 'make', 'binari', 'linear', 'classifi', 'although', 'method', 'platt', 'scale', 'exist', 'use', 'svm', 'probabilist', 'classif', 'set', 'svm', 'model', 'represent', 'exampl', 'point', 'space', 'map', 'exampl', 'separ', 'categori', 'divid', 'clear', 'gap', 'wide', 'possibl', 'new', 'exampl', 'map', 'space', 'predict', 'belong', 'categori', 'base', 'side', 'gap', 'fall', 'addit', 'perform', 'linear', 'classif', 'svms', 'effici', 'perform', 'classif', 'use', 'call', 'kernel', 'trick', 'implicit', 'map', 'input', 'featur', 'space', 'data', 'unlabel', 'supervis', 'learn', 'possibl', 'unsupervis', 'learn', 'approach', 'requir', 'attempt', 'find', 'natur', 'cluster', 'data', 'group', 'map', 'new', 'data', 'form', 'group', 'cluster', 'algorithm', 'creat', 'hava', 'siegelmann', 'vladimir', 'vapnik', 'appli', 'statist', 'support', 'vector', 'develop', 'support', 'vector', 'machin', 'algorithm', 'categor', 'unlabel', 'data', 'one', 'wide', 'use', 'cluster', 'algorithm', 'industri', 'applic']\n",
      "\n",
      "['statist', 'logist', 'model', 'logit', 'model', 'use', 'model', 'probabl', 'certain', 'class', 'event', 'exist', 'extend', 'model', 'sever', 'class', 'event', 'determin', 'whether', 'imag', 'contain', 'cat', 'dog', 'lion', 'etc', 'object', 'detect', 'imag', 'would', 'assign', 'probabl', 'sum', 'one', 'logist', 'regress', 'statist', 'model', 'basic', 'form', 'use', 'logist', 'function', 'model', 'binari', 'depend', 'variabl', 'although', 'mani', 'complex', 'extens', 'exist', 'regress', 'analysi', 'logist', 'regress', 'logit', 'regress', 'estim', 'paramet', 'logist', 'model', 'form', 'binari', 'regress', 'mathemat', 'binari', 'logist', 'model', 'depend', 'variabl', 'two', 'possibl', 'valu', 'repres', 'indic', 'variabl', 'two', 'valu', 'label', 'logist', 'model', 'logarithm', 'odd', 'valu', 'label', 'linear', 'combin', 'one', 'independ', 'variabl', 'predictor', 'independ', 'variabl', 'binari', 'variabl', 'two', 'class', 'code', 'indic', 'variabl', 'continu', 'variabl', 'ani', 'real', 'valu', 'correspond', 'probabl', 'valu', 'label', 'vari', 'certain', 'valu', 'certain', 'valu', 'henc', 'label', 'function', 'convert', 'probabl', 'logist', 'function', 'henc', 'name', 'unit', 'measur', 'scale', 'call', 'logit', 'logist', 'unit', 'henc', 'altern', 'name', 'analog', 'model', 'differ', 'sigmoid', 'function', 'instead', 'logist', 'function', 'also', 'use', 'probit', 'model', 'defin', 'characterist', 'logist', 'model', 'increas', 'one', 'independ', 'variabl', 'multipl', 'scale', 'odd', 'given', 'outcom', 'constant', 'rate', 'independ', 'variabl', 'paramet', 'binari', 'depend', 'variabl', 'general', 'odd', 'ratio', 'binari', 'logist', 'regress', 'model', 'depend', 'variabl', 'two', 'level', 'categor', 'output', 'two', 'valu', 'model', 'multinomi', 'logist', 'regress', 'multipl', 'categori', 'order', 'ordin', 'logist', 'regress', 'exampl', 'proport', 'odd', 'ordin', 'logist', 'model', 'logist', 'regress', 'model', 'simpli', 'model', 'probabl', 'output', 'term', 'input', 'doe', 'perform', 'statist', 'classif', 'classifi', 'though', 'use', 'make', 'classifi', 'instanc', 'choos', 'cutoff', 'valu', 'classifi', 'input', 'probabl', 'greater', 'cutoff', 'one', 'class', 'cutoff', 'common', 'way', 'make', 'binari', 'classifi', 'coeffici', 'general', 'comput', 'express', 'unlik', 'linear', 'least', 'squar', 'see', 'model', 'fit', 'logist', 'regress', 'general', 'statist', 'model', 'origin', 'develop', 'popular', 'primarili', 'joseph', 'berkson', 'begin', 'berkson', 'coin', 'logit', 'see', 'histori']\n",
      "\n",
      "\n",
      "To idx corpus:\n",
      "\n",
      "[0, 1, 2, 3, 4, 5, 6, 3, 7, 8, 9, 4, 10, 11, 12, 13, 14, 15, 0, 16, 17, 18, 19, 20, 21, 22, 3, 0, 0, 23, 24, 25, 26, 27, 28, 29, 30, 31, 28, 29, 32, 33, 34, 35, 36, 37, 38, 3, 39, 40, 0, 41, 42, 43, 44, 45, 46, 47, 48, 49, 41, 14, 50, 8, 51, 52, 53, 0, 0, 46, 54, 0, 0, 48, 55, 56, 57, 3, 58, 55, 59, 60, 61, 62, 63, 64, 3, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 0, 83, 16, 3, 84, 85, 48, 2, 25, 26, 86, 87, 27, 88, 3, 89, 90, 91, 92, 3, 93, 94, 89, 95, 96, 13, 97, 98, 99, 90, 100, 101, 3, 0, 102, 103, 101, 3, 104, 105, 0, 106, 102, 107, 108, 104, 0, 109, 110, 2, 111, 112, 71, 113, 104, 0, 114, 115, 83, 113, 116, 117, 0, 118, 119, 120, 0, 121, 84, 122, 120, 0, 123, 124, 3, 99, 125, 53, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 0, 137, 101, 3, 104, 140, 141, 137, 0, 142, 27, 143, 137, 104, 105, 144, 145, 146, 0, 147, 148, 137, 104, 105, 87, 27, 145, 0, 149, 150, 55, 151, 152, 153, 0, 154, 155, 152, 52, 156, 39, 39, 157, 158, 159, 160, 161, 17, 162, 163, 164, 162, 165, 166, 167, 168, 162, 169, 170, 171, 172, 173, 156, 0, 174, 175, 176, 158, 177, 178, 153, 179, 180, 28, 181, 0, 182, 183, 184, 185, 186, 187, 172, 188, 183, 45, 184, 118, 189, 183, 190, 53, 188, 185, 191, 192, 193, 0, 194, 195, 194, 196, 197, 55]\n",
      "\n",
      "[198, 199, 200, 1, 2, 3, 201, 27, 137, 145, 202, 203, 3, 204, 205, 206, 137, 207, 3, 37, 3, 51, 48, 201, 208, 209, 210, 209, 211, 212, 213, 211, 1, 18, 3, 214, 1, 134, 215, 216, 145, 27, 201, 87, 134, 134, 198, 217, 218, 8, 219, 220, 8, 221, 222, 8, 223, 224, 41, 225, 226, 3, 227, 228, 229, 230, 231, 224, 232, 41, 110, 219, 221, 233, 8, 2, 231, 234, 235, 43, 204, 236, 198, 237, 238, 239, 240, 3, 27, 241, 242, 243, 155, 8, 244, 231, 3, 245, 246, 199, 247, 27, 198, 247, 248, 249, 247, 124, 250, 42, 251, 18, 56, 161, 252, 253, 198, 123, 1, 254, 211, 6, 255, 199, 256, 257, 258, 143, 259, 198, 136, 55, 55, 260, 261, 262, 263, 264, 265, 3, 266, 191, 198, 123, 267, 268, 191, 199, 269, 210, 198, 270, 271, 199, 272, 271, 198, 273, 1, 271, 201, 274, 123, 191, 199, 1, 35, 199, 275, 198, 276, 123, 1, 201, 198, 55, 3, 131, 277, 76, 120, 278, 279, 280, 78, 281, 282, 128, 129, 283, 284, 79, 285, 76, 286, 287, 288, 198, 289, 290, 291, 292, 293, 96, 115, 294, 295, 82, 197, 296, 297, 298, 98, 299, 284, 51, 172, 300, 301, 302, 303, 304, 305, 306, 307, 198, 114, 109, 308, 309, 310, 198, 311, 198, 312, 309, 198, 313, 198, 314, 309, 198, 315, 109, 198, 309, 316, 317, 109, 69, 318, 319, 198, 198, 121, 84, 320, 321, 155, 62, 322, 104, 48, 297, 323, 55, 195, 324, 325, 294, 198, 326, 297, 43, 90, 327, 328, 3, 329, 31, 330, 128, 331, 332, 333, 69, 334, 115, 335, 336, 337, 198, 338, 339, 340, 341, 149, 125, 55, 151, 152, 183, 342, 156, 243, 213, 198, 198, 53, 343, 344, 171, 172, 211, 345, 346, 347, 213, 348, 349, 350, 351, 104, 198, 270, 349, 271, 352, 184, 198, 264, 353, 198, 178, 198, 354, 153, 355, 356, 201, 198, 182, 183, 184, 185, 357, 186, 187, 172, 358, 45, 184, 359, 183, 360, 183, 199, 161, 183, 361, 183, 198, 118, 189, 183, 308, 362, 363, 364, 365, 198, 209, 185, 198, 209, 366, 198, 367, 198, 368]\n",
      "\n",
      "[369, 141, 370, 371, 372, 373, 200, 1, 2, 3, 374, 329, 131, 284, 25, 26, 375, 265, 218, 376, 8, 377, 11, 378, 14, 379, 8, 380, 381, 369, 124, 382, 250, 123, 1, 374, 26, 371, 76, 131, 69, 331, 383, 161, 384, 69, 385, 386, 387, 69, 388, 389, 390, 53, 391, 18, 55, 260, 3, 392, 331, 393, 394, 395, 369, 396, 144, 271, 371, 397, 398, 399, 144, 371, 400, 144, 401, 347, 402, 161, 3, 26, 403, 404, 405, 260, 406, 407, 408, 409, 410, 411, 230, 412, 413, 414, 415, 3, 416, 84, 131, 417, 48, 3, 104, 206, 25, 170, 418, 419, 420, 57, 421, 21, 369, 422, 423, 424, 337, 369, 99, 2, 115, 304, 425, 375, 104, 374, 426, 425, 161, 3, 26, 427, 428, 55, 429, 430, 369, 204, 3, 106, 431, 298, 432, 433, 206, 434, 57, 435, 142, 98, 436, 18, 437, 415, 369, 83, 204, 98, 106, 438, 80, 206, 146, 145, 336, 207, 3, 37, 439, 146, 440, 441, 442, 443, 142, 27, 444, 87, 27, 369, 445, 137, 216, 130, 446, 447, 371, 46, 448, 216, 130, 449, 450, 451, 452, 3, 369, 453, 2, 3, 204, 205, 137, 207, 3, 37, 198, 454, 455, 84, 369, 456, 3, 374, 3, 102, 457, 287, 285, 71, 113, 131, 288, 458, 459, 460, 461, 27, 3, 462, 463, 31, 136, 369, 464, 372, 463, 465, 45, 466, 467, 215, 281, 46, 468, 54]\n",
      "\n",
      "[454, 469, 470, 1, 3, 471, 3, 472, 216, 145, 87, 27, 137, 473, 216, 3, 204, 205, 474, 475, 476, 198, 477, 87, 27, 474, 475, 476, 144, 136, 478, 479, 378, 245, 8, 480, 214, 265, 481, 454, 134, 51, 1, 1, 3, 471, 482, 46, 483, 469, 484, 454, 51, 277, 84, 131, 26, 265, 54, 485, 486, 130, 487, 454, 142, 488, 88, 204, 205, 237, 277, 71, 72, 489, 76, 73, 120, 131, 69, 490, 491, 48, 55, 16, 57, 3, 454, 114, 18, 492, 493, 494, 454, 495, 454, 235, 84, 496, 115, 497, 336, 498, 499, 454, 500, 16, 25, 26, 13, 84, 291, 501, 502, 503, 104, 99, 10, 504, 505, 16, 16, 469, 506, 507, 508, 509, 454, 51, 510, 3, 55, 206, 511, 512, 343, 513, 514, 55, 81, 515, 516, 517, 518, 519, 515, 209, 520, 295, 521, 522, 523, 524, 51, 99, 16, 90, 525, 526, 469, 132, 7, 527, 528, 469, 132, 529, 513, 163, 469, 529, 530, 531, 532, 533, 534, 469, 535, 536, 454, 537, 538, 539, 540, 541, 542, 454, 73, 454, 543, 53, 544, 545, 546, 547, 230, 161, 548, 549, 171, 172, 550, 54, 163, 156, 454, 163, 193, 551, 366, 343, 552, 553, 243, 21, 45, 185, 347, 21, 149, 471, 161, 554, 230, 45, 183, 555, 556, 69, 43, 557, 558, 559, 560, 52, 125, 69, 183, 560, 183, 561, 562, 183, 183, 53, 69, 183, 563, 564, 18, 183, 565, 156, 566, 567, 48, 429, 568, 3]\n",
      "\n",
      "[569, 570, 571, 572, 200, 1, 2, 3, 573, 135, 137, 207, 463, 37, 27, 569, 142, 27, 574, 575, 3, 463, 37, 137, 443, 216, 130, 137, 142, 27, 204, 205, 216, 130, 336, 569, 87, 27, 576, 473, 265, 137, 577, 104, 578, 1, 3, 476, 48, 557, 579, 580, 569, 245, 224, 501, 502, 579, 581, 8, 582, 578, 583, 584, 27, 570, 585, 110, 578, 586, 290, 104, 587, 588, 589, 590, 569, 591, 57, 3, 592, 56, 74, 70, 76, 72, 69, 90, 252, 71, 81, 569, 48, 18, 570, 3, 593, 594, 591, 57, 3, 48, 16, 84, 26, 595, 596, 25, 281, 48, 2, 149, 597, 104, 579, 580, 49, 598, 599, 93, 289, 216, 95, 92, 93, 573, 92, 501, 502, 92, 104, 600, 197, 293, 94, 601, 602, 603, 227, 3, 573, 569, 3, 3, 503, 104, 604, 455, 605, 606, 607, 608, 454, 609, 298, 569, 134, 112, 88, 610, 611, 612, 101, 134, 573, 88, 613, 370, 423, 614, 611, 612, 3, 615, 616, 106, 617, 395, 55, 513, 618, 286, 573, 43, 569, 27, 51, 277, 26, 265, 482, 46, 483, 10, 209, 569, 48, 195, 3, 55, 204, 1, 3, 607, 619, 53, 204, 1, 42, 620, 621, 622, 623, 58, 624, 625, 440, 626, 627, 628, 629, 199, 630, 42, 631, 632, 42, 633, 634, 635, 636, 3, 337, 277, 84, 131, 214, 573]\n",
      "\n",
      "[637, 638, 253, 639, 640, 18, 616, 641, 642, 643, 644, 645, 646, 110, 647, 172, 648, 649, 650, 651, 652, 637, 653, 48, 18, 654, 188, 655, 656, 48, 657, 101, 528, 658, 188, 655, 659, 637, 660, 661, 662, 637, 448, 663, 663, 664, 637, 626, 637, 172, 665, 666, 667, 600, 668, 669, 670, 671, 638, 672, 673, 674, 468, 637]\n",
      "\n",
      "[675, 676, 677, 678, 679, 680, 200, 681, 682, 258, 675, 676, 683, 684, 685, 686, 687, 688, 677, 678, 265, 375, 689, 690, 265, 691, 502, 135, 216, 145, 137, 265, 55, 675, 692, 640, 381, 693, 14, 694, 695, 191, 692, 696, 381, 697, 698, 697, 699, 700, 701, 50, 693, 15, 1, 3, 702, 137, 3, 703, 704, 705, 3, 51, 31, 706, 626, 707, 708, 709, 710, 675, 145, 711, 712, 713, 23, 676, 45, 214, 714, 8, 715, 716, 717, 27, 718, 719, 720, 42, 721, 262, 622, 675, 722, 116, 693, 723, 124, 724, 725, 339, 42, 440, 641, 639, 726, 116, 720, 45, 727, 728, 729, 502, 730, 731, 365, 42, 53, 675, 732, 68, 733, 724, 734, 35, 676, 264, 731, 124, 729, 260, 192, 45, 266, 18, 735, 209, 389, 14, 50, 736, 737, 675, 260, 738, 739, 370, 740, 741, 50, 675, 192, 14, 676, 264, 14, 50, 675, 742, 389, 209, 743, 682, 46, 265, 123, 84, 277, 744, 389, 487, 745, 629, 84, 277, 746, 747, 676, 748, 55, 277, 57, 278, 749, 726, 746, 675, 750, 115, 88, 431, 452, 653, 83, 226, 265, 55, 751, 752, 753, 172, 754, 755, 756, 757, 122, 758, 759, 760, 761, 762, 172, 326, 763, 764, 765, 760, 761, 766, 737, 767, 768, 769, 675, 770, 771, 772, 265, 773, 90, 537, 5, 45, 514, 528, 375, 774, 45, 775, 84, 776, 777, 362, 55, 252, 515, 778]\n",
      "\n",
      "[779, 380, 780, 22, 781, 782, 783, 200, 1, 2, 3, 784, 785, 786, 715, 646, 787, 788, 224, 789, 265, 3, 790, 191, 791, 8, 230, 792, 793, 779, 246, 792, 794, 795, 290, 240, 26, 796, 797, 798, 15, 799, 15, 800, 801, 802, 803, 804, 805, 713, 779, 27, 760, 806, 337, 807, 721, 762, 189, 3, 808, 809, 810, 802, 22, 258, 721, 811, 630, 812, 446, 813, 22, 256, 3, 814, 815, 816, 817, 818, 802, 819, 380, 256, 820, 821, 123, 31, 804, 822, 192, 380, 628, 823, 824, 380, 256, 804, 123, 16, 463, 84, 131, 825, 291, 810, 826, 779, 134, 1, 827, 828, 826, 256, 829, 830, 104, 380, 628, 551, 779, 831, 1, 143, 380, 209, 832, 415, 833, 391, 834, 835, 84, 431, 779, 2, 3, 204, 205, 210, 836, 787, 503, 352, 837, 224, 352, 796, 788, 14, 203, 143, 106, 204, 1, 838, 839, 840, 454, 784, 841, 3, 779, 114, 230, 16, 468, 109, 131, 113, 842, 506, 3, 843, 556, 764, 844, 235, 845, 125, 846, 125, 16, 55, 847, 785, 207, 48, 28, 848, 849, 160, 850, 851, 852, 779, 853, 556, 414, 727, 28, 172, 854, 178, 158, 855, 626, 48, 856, 857, 858, 616, 859, 55, 860, 861, 862, 779, 104, 863, 1, 62, 864, 160, 779, 172, 865, 55, 151, 427, 866, 867, 380, 256, 564, 44, 868, 568, 380, 869, 160, 131, 299, 375, 104, 779, 114, 45, 366, 870, 784, 45, 156, 131, 871, 872, 45, 873, 874, 875, 45, 193, 45, 708, 183, 802, 876, 183, 682, 69, 172, 230, 877, 878, 71, 879, 880, 389, 137, 779, 813, 365, 42, 881, 326, 779, 99, 216, 483, 207, 272, 111, 265, 137, 207, 882, 265, 272, 142, 27, 779, 99, 216, 483, 207, 52, 779, 3, 830, 789, 779, 830, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 830, 894, 895, 576, 265, 272, 87, 27, 779, 279, 68, 48, 896, 897, 645, 170, 898, 90, 172, 899, 900, 87, 134, 779, 145, 137, 216]\n",
      "\n",
      "[901, 902, 903, 27, 904, 902, 905, 906, 907, 908, 909, 910, 489, 911, 912, 913, 914, 915, 901, 902, 48, 125, 916, 904, 902, 917, 593, 918, 902, 919, 902, 901, 902, 920, 918, 902, 921, 922, 923, 924, 921, 910, 925, 926, 927, 928, 929, 930, 931, 932, 272, 933, 934, 935, 489, 936, 209, 110, 937, 938, 939, 940, 592, 230, 901, 902, 941, 942, 448, 943, 410, 944, 227, 920, 945, 943, 410, 946, 901, 902, 941, 947, 948, 935, 949, 950, 951, 940, 952, 457, 953, 949, 946, 567, 954]\n",
      "\n",
      "[918, 902, 904, 902, 955, 902, 956, 957, 958, 496, 279, 959, 923, 960, 956, 922, 961, 962, 653, 963, 961, 959, 918, 902, 959, 923, 653, 958, 964, 936, 965, 60, 496, 966, 51, 58, 967, 968, 918, 902, 941, 969, 961, 962, 970, 960, 956, 448, 957, 971, 959, 972, 973, 644, 941, 926, 974, 975, 922, 976, 977, 978, 902, 941, 517, 961, 962, 976, 785, 979, 639, 980, 981, 982, 664, 955, 760, 983, 984, 985, 986, 987, 902]\n",
      "\n",
      "[919, 902, 988, 904, 902, 546, 989, 990, 991, 962, 963, 922, 992, 760, 918, 993, 918, 902, 994, 203, 448, 962, 919, 902, 51, 31, 644, 951, 995, 996, 958, 110, 48, 125, 227, 997, 904, 902, 557, 918, 901, 902, 902, 772, 248, 203, 448, 918, 919, 944, 18, 227, 946, 448, 919, 902, 998, 999, 1000, 1001, 1000, 1001, 448, 919, 902, 765, 1002, 1003, 692, 1004, 911, 1005, 941, 1006, 1000, 1001, 1007, 904, 902, 765, 962, 922, 1008, 1009, 927, 1010, 1011, 1000, 1001, 1012, 1013, 962, 1014, 279, 1015, 1016, 1013, 971, 1017, 962, 1018, 1019, 1020, 1021, 962, 586, 1022, 86, 765, 476, 978, 58, 919, 902, 1023, 902, 971, 229, 1024, 631, 1025, 1026, 1024, 912, 964, 956, 1027, 1028, 956, 1029, 1030, 1031, 1032, 919, 902, 909, 1033, 1031, 1032, 592, 1034, 1035, 1036, 1037, 912, 1038, 31, 998, 999, 1001, 1039, 909, 1003, 919, 902, 1040, 958, 962, 1041, 1042, 956, 1027, 1043, 956, 1030, 749, 962, 31, 1044, 246, 1045, 919, 902, 1046, 996, 135, 1047, 1048, 919, 902, 282, 230, 1049, 1050, 1051, 1052, 962, 229, 1053, 993, 918, 902, 1054, 1055, 918, 902, 1056, 960, 1057, 995, 1058, 1057, 922, 958, 962, 919, 902, 1056, 960, 1059, 995, 1058, 1030, 1060, 197, 51, 448, 918, 902, 1048, 51, 119, 919, 901, 944]\n",
      "\n",
      "[1044, 988, 1061, 1062, 197, 448, 902, 1063, 962, 1064, 919, 1065, 1066, 1044, 902, 1067, 1068, 963, 962, 936, 1069, 1070, 961, 197, 1071, 968, 1072, 557, 1070, 352, 1073, 352, 1074, 1044, 1075, 1030, 1076, 1068, 1067, 34, 1077, 1026, 958, 1078, 626, 1079, 248, 1080, 916, 951, 1066, 1081, 902, 1067, 958, 948, 448, 1082, 959, 1083, 1044, 1084, 1085, 1086, 1044, 1087, 1088, 902, 1067, 262, 1089, 955, 1090, 1044, 414, 1045, 1030, 951, 1044, 1088, 448, 1091, 230, 119, 1092, 752, 1093, 1094, 1095, 169, 1096]\n",
      "\n",
      "[1000, 946, 965, 1097, 1026, 968, 939, 1066, 1098, 1099, 1000, 1099, 1100, 1000, 1101, 169, 1000, 214, 1000, 1102, 289, 1103, 1000, 749, 1098, 962, 1024, 1104, 1105, 1000, 1106, 1038, 28, 1107, 1043, 1083, 1107, 1043, 445, 1108, 1109, 1092, 169, 972, 28, 1110, 1055, 476, 1111, 1112, 1106, 1107, 1043, 977, 1113, 1107, 1114, 21, 448, 1092, 1115, 1108, 1116, 1063, 1117, 941, 1118, 1119, 468, 1120, 994, 1039, 941, 1121, 1122, 1058, 1123, 1124, 1125, 1018, 1126, 1122, 1121, 951, 448, 1000, 214, 951, 962, 1116, 1000, 1127, 929, 1000, 1128, 1129, 1130, 1131, 644, 1000, 920, 1132, 941, 1133, 1006, 1134, 1008, 297, 904, 902, 944, 1089, 985, 1135, 235, 626, 119, 1134, 1008, 1000, 214, 1136, 1008, 971, 962, 1080, 1000, 31, 1101, 1102, 1008, 1137, 941]\n",
      "\n",
      "[1047, 1034, 1138, 1034, 1018, 951, 1006, 1139, 1140, 48, 1141, 1142, 51, 31, 1143, 724, 1142, 1144, 48, 1141, 1142, 58, 1145, 1034, 1138, 48, 939, 58, 1041, 1034, 1138, 1146, 1147, 1148, 1034, 1138, 1041, 1149, 1143, 1142, 1150, 1151, 1152, 1139, 1034, 1138, 1006, 951, 448, 1034, 1153, 956, 1154, 1155, 951, 1156, 135, 962, 951, 58, 1034, 951, 1013, 1057, 169, 1140, 1157, 966, 1141, 1142, 1153, 948, 1158, 956, 966, 377, 1013, 1057, 1112, 1159, 448, 1160, 110, 1138, 1001, 1034, 1138, 928, 1057, 995, 1058, 1140, 1157, 966, 1153, 1151, 1161, 995, 1058, 1142, 1049, 1148, 1001, 1034, 1138, 124, 988, 1138, 1001, 1162, 1163, 448, 739, 1164, 1045, 592, 951, 1143, 1034, 1155, 1156, 1165, 1022, 951, 772, 1156, 592, 1047, 1082, 749, 135, 1165, 974, 1034, 1138, 230, 1164, 448, 1045, 1166, 48, 35, 18, 1167, 997, 1168, 1150, 1169, 1110, 1070, 1034, 1138, 448, 1022, 1150, 951, 1099, 962, 963, 966, 1140, 1141, 1142, 80, 951, 49, 966, 1141, 1142, 1170, 1171, 1172, 1140, 966, 1022, 951, 448, 203, 1150, 1140, 1168, 1052, 1090, 1140, 1142, 1004, 1090, 1141, 1142, 1034, 1138, 1001, 119, 1173, 1174, 1006, 1140, 1141, 1142, 1175, 974, 1176, 1141, 1142, 663, 1034, 1006, 1140, 1012, 1177, 1141, 1142, 663, 149, 1178, 861, 1138, 951, 985, 1022, 448, 807, 28, 1018, 663, 51, 1022, 639, 1106, 1179, 1022, 1180, 807, 1181, 1182, 1138, 1106, 1183, 1184, 807, 28, 1185, 956, 1186, 1138, 1187, 1188, 1187, 1189, 807, 28, 1018, 448, 1022, 951, 1034, 951, 614, 401, 1146, 807, 28, 1034, 951, 34, 603, 1190]\n",
      "\n",
      "[904, 902, 904, 1191, 51, 197, 918, 902, 951, 528, 902, 941, 969, 962, 448, 1089, 1138, 1001, 80, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1197, 1194, 1197, 924, 48, 1199, 1150, 946, 279, 1047, 902, 1200, 1201, 1202, 444, 1194, 1203, 1194, 1157, 963, 961, 959, 1204, 1100, 48, 18, 997, 1205, 961, 941, 347, 951, 1206, 971, 959, 48, 997, 203, 1207, 1034, 1008, 401, 946, 1208, 1209, 1080, 448, 1205, 1210, 1089, 963, 1205, 951, 1067, 959, 586, 1024, 957, 959, 1211, 997, 266, 1212, 1213, 1214, 1077, 971, 959, 957, 1024, 1150, 1100, 997, 279, 352, 1213, 1166, 49, 85, 1034, 1089, 1191, 1063, 85, 1089, 448, 58, 1215, 1216, 1217, 957, 958, 229, 1024, 962, 1218, 918, 902, 1077, 919, 902, 1018, 978, 1219, 929, 118, 1000, 962, 765, 957, 971, 962, 110, 765, 1000, 941, 1220, 1221, 1222, 1223, 1194, 119, 1047, 741, 965, 80, 741, 965, 904, 941, 1009, 1218, 962, 48, 1214, 448, 1000, 941, 284, 1045]\n",
      "\n",
      "[1047, 1224, 951, 1225, 951, 448, 951, 995, 1226, 975, 187, 1080, 574, 951, 1079, 975, 187, 974, 1176, 1227, 149, 1228, 1229, 1230, 1231, 964, 1020, 1227, 445, 1206, 995, 1232, 48, 1224, 1138, 1047, 951, 916, 110, 448, 1224, 956, 951, 1207, 1143, 1142, 401, 230, 294, 739, 1080, 1138, 1001, 1224, 1138, 1225, 1138, 135, 1156, 1224, 951, 110, 1207, 1138, 950, 1207, 1224, 951, 1143, 1142, 18, 1077, 966, 576, 710, 1142, 18, 966, 922, 1224, 951, 1233, 1234, 966, 922, 1034, 1235, 48, 724, 1142, 1153, 724, 1142, 1207, 1142, 18, 975, 1064, 710, 1142, 1236, 1142, 101, 494, 966, 138, 995, 966, 922, 1237, 1226, 966, 1226, 966, 1078, 922, 956, 1238, 995, 1224, 956, 1078, 626, 5, 89, 1209, 58, 1225, 1224, 5, 1078, 1239, 626, 1240, 951, 920, 1042, 956, 927, 1224, 956, 51, 448, 1241, 951, 165, 1025, 1224, 951, 326, 48, 724, 1142, 1041, 1209, 1234, 1157, 1242, 1243, 452, 724, 1142, 1156, 1207, 1143, 1142, 517, 1234, 1244, 1207, 1224, 1138, 951, 1143, 1142, 18, 465, 1009, 496, 18, 966, 951, 1245, 1224, 1138, 1041, 997, 911, 1246, 1224, 1138, 959, 1247, 1234, 1246, 1224, 951, 1224, 1138, 951, 1248, 951, 995, 496, 1146, 958, 1249, 85, 1047, 1089, 1008, 1048, 448, 203, 1008, 977, 1250, 1251, 966, 1008, 958, 995, 139, 1251, 48, 975, 1251, 1013, 639, 203, 1207, 1008, 1252, 517, 1115, 1253, 1254, 1034, 807, 28, 980, 951, 1022, 1224, 1138, 517, 1047, 951, 1026, 80, 297, 1255, 855, 1256, 808, 1256, 1257, 1225, 980, 189]\n",
      "\n",
      " [-----------------100%-----------------] 3001 of 3000 complete in 1006.9 sec"
     ]
    }
   ],
   "source": [
    "lda_model = LatentDirichletAllocation(no_topics=2, initial_corpus=DOCUMENTS, alpha=1.0, beta=1.0, show_tokens=True, show_idxTokens=True)\n",
    "lda_model.fit(no_iter=3000, burn=300, burn_till_tuned=0, extraction_type=\"last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 - Can the topic model be used to define a topic-based similarity measure between documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess topic-based similarity between two documents, we can get a score with respect to all the topics available, obtaining a distribution of probabilities for each of them (which does not automatically sum up to 1, but it can be normalized by dividing over the sum of all topics). Using these probabilities distributions, we can calculate their score similiarity based on 2 methods (which are not exhaustive, but are well-known in literature for their efficiency): KL Divergence (asymmetrical) and Jensen-Shannon Divergence (symmetrical):\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\displaystyle D_{\\text{KL}}(P\\parallel Q)=\\sum _{x\\in {\\mathcal {X}}}P(x)\\log \\left({\\frac {P(x)}{Q(x)}}\\right)} (1)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "{{\\rm {JSD}}}(P\\parallel Q)={\\frac  {1}{2}}D_{\\text{KL}}(P\\parallel M)+{\\frac  {1}{2}}D_{\\text{KL}}(Q\\parallel M), M={\\frac  {1}{2}}(P+Q) (2)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print representative words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representative words for each topic:\n",
      "topic_0 : ['largest', 'unit', 'kingdom', 'km', 'estuari', 'lead', 'sea', 'found', 'roman', 'core', 'centr', 'squar', 'mile', 'retain', 'boundari', 'follow', 'mediev', 'inner', 'locat', 'much', 'nation', 'addit', 'south', 'modern', 'mayor', 'global', 'power', 'desir', 'expens', 'sustain', 'exert', 'art', 'commerc', 'fashion', 'healthcar', 'develop', 'rank', 'econom', 'metropolitan', 'measur', 'passeng', 'invest', 'host', 'billionair', 'moscow', 'individu', 'univers', 'form', 'institut', 'high', 'imperi', 'colleg', 'natur', 'becam', 'first', 'rang', 'languag', 'region', 'municip', 'popul', 'greater', 'uk', 'urban', 'third', 'pari', 'inhabit', 'belt', 'contain', 'tower', 'palac', 'abbey', 'observatori', 'defin', 'prime', 'includ', 'buckingham', 'eye', 'circus', 'paul', 'numer', 'museum', 'galleri', 'librari', 'sport', 'british', 'west', 'underground', 'oldest', 'railway', 'network', 'german', 'germani', 'european', 'union', 'accord', 'within', 'feder', 'state', 'brandenburg', 'contigu', 'six', 'million', 'straddl', 'havel', 'tributari', 'elb', 'western', 'spandau', 'topograph', 'mani', 'southeastern', 'müggelse', 'due', 'plain', 'season', 'climat', 'park', 'variant', 'marchian', 'document', 'trade', 'margravi', 'prussia', 'empir', 'weimar', 'republ', 'reich', 'war', 'ii', 'subsequ', 'divid', 'de', 'facto', 'wall', 'east', 'territori', 'reunif', 'onc', 'base', 'firm', 'sector', 'industri', 'facil', 'serv', 'air', 'rail', 'complex', 'tourist', 'signific', 'pharmaceut', 'biomed', 'engin', 'clean', 'tech', 'construct', 'electron', 'universität', 'zu', 'technisch', 'freie', 'free', 'udk', 'zoolog', 'zoo', 'movi', 'product', 'nightlif', 'qualiti', 'live', 'seen', 'emerg', 'gate', 'reichstag', 'build', 'murder', 'jew', 'side', 'column', 'tallest', 'orchestra', 'old', 'bode', 'pergamon', 'schedul', 'opera', 'marathon', 'bucharest', 'bucurești', 'less', 'mi', 'danub', 'bulgarian', 'mention', 'mix', 'nouveau', 'interbellum', 'deco', 'era', 'eleg', 'sophist', 'elit', 'earn', 'nicknam', 'although', 'district', 'damag', 'earthquak', 'program', 'systemat', 'renov', 'recent', 'experienc', 'boom', 'cbre', 'techcrunch', 'uipath', 'billion', 'mastercard', 'past', 'decreas', 'ad', 'satellit', 'town', 'use', 'report', 'rate', 'rome', 'prosper', 'shop', 'administr', 'counti', 'approxim', 'monocentr', 'sq', 'manzanar', 'communiti', 'seat', 'resid', 'monarch', 'lui', 'agglomer', 'environ', 'contribut', 'status', 'output', 'iberian', 'peninsula', 'southern', 'vast', 'compani', 'telefónica', 'magazin', 'organ', 'secretariat', 'oei', 'board', 'regul', 'promot', 'associ', 'fundéu', 'organis', 'fitur', 'arco', 'simo', 'tci', 'possess', 'feel', 'neighbourhood', 'street', 'restor', 'buen', 'archiv', 'golden', 'del', 'reina', 'sofía', 'hold', 'becom', 'lisbon', 'portugues', 'lisboa', 'portug', 'mainland', 'atlant', 'ocean', 'metro', 'point', 'cabo', 'da', 'becaus', 'alongsid', 'grow', 'port', 'motorway', 'alfa', 'barcelona', 'milan', 'athen', 'venic', 'florenc', 'ppp', 'per', 'capita', 'amount', 'occupi', 'gross', 'multin', 'julia', 'captur', 'crusad', 'afonso', 'henriqu', 'reconqu', 'road', 'way', 'land', 'pave', 'improv', 'allow', 'convey', 'motor', 'cart', 'carriageway', 'verg', 'bike', 'may', 'parkway', 'freeway', 'primari', 'secondari', 'vietnam', 'nam', 'cộng', 'chủ', 'nghĩa', 'asia', 'indochines', 'share', 'china', 'lao', 'thailand', 'gulf', 'philippin', 'former', 'indic', 'earli', 'coastal', 'annex', 'dynasti', 'bc', 'divis', 'indochina', 'colonis', 'french', 'born', 'franc', 'rival', 'intensifi', 'saw', 'extens', 'intervent', 'reunifi', 'isol', 'initi', 'reform', 'facilit', 'result', 'enjoy', 'challeng', 'pollut', 'inadequ', 'welfar', 'poor', 'human', 'persecut', 'advoc', 'restrict', 'liberti', 'diplomat', 'member', 'asian', 'asean', 'wto', 'eponym', 'vitosha', 'built', 'humid', 'midway', 'black', 'adriat', 'closest', 'antiqu', 'sredet', 'begin', 'serdi', 'raid', 'visigoth', 'avar', 'slav', 'khan', 'krum', 'byzantin', 'reincorpor', 'literari', 'provinc', 'next', 'lyulin', 'la', 'vella', 'primat', 'toler', 'fact', 'sveta', 'nedelya', 'bashi', 'synagogu', 'cathol', 'joseph', 'top', 'especi', 'afford', 'boyana', 'onto', 'ballet', 'vasil', 'levski', 'stadium', 'ivan', 'vazov', 'amphitheatr', 'visitor', 'lifestyl', 'began', 'percentil', 'dragoman', 'slivnitsa', 'kostinbrod', 'bozhurisht', 'gorna', 'malina', 'ihtiman', 'kostenet', 'pernik', 'hour', 'car', 'dimitrovgrad', 'serbia', 'reinforc', 'learn', 'rl', 'concern', 'softwar', 'maxim', 'notion', 'cumul', 'basic', 'paradigm', 'supervis', 'unsupervis', 'need', 'pair', 'explicit', 'focus', 'find', 'balanc', 'current', 'markov', 'decis', 'context', 'dynam', 'classic', 'method', 'exact', 'target', 'mdps', 'infeas', 'task', 'function', 'input', 'infer', 'data', 'set', 'analyz', 'instanc', 'anim', 'often', 'refer', 'type', 'undetect', 'minimum', 'contrast', 'usual', 'densiti', 'categori', 'compon', 'segment', 'attribut', 'extrapol', 'branch', 'categor', 'identifi', 'common', 'react', 'presenc', 'absenc', 'strategi', 'space', 'characterist', 'origin', 'generat', 'matrix', 'linear', 'algebra', 'eigendecomposit', 'oper', 'varianc', 'sigmoid', 'applic', 'field', 'though', 'involv', 'could', 'say', 'condit', 'distribut', 'adversari', 'artifici', 'neural', 'code', 'manner', 'aim', 'ignor', 'nois', 'reconstruct', 'reduc', 'possibl', 'henc', 'exist', 'regular', 'spars', 'proven', 'effect', 'solv', 'problem', 'recognit', 'quantize', 'voronoi', 'cell', 'difficult', 'error', 'better', 'solut', 'comput', 'howev', 'converg', 'optimum', 'employ', 'tend', 'spatial', 'extent', 'neighbor', 'confus', 'rocchio', 'scalar', 'explanatori', 'depend', 'simpl', 'multivari', 'correl', 'rather', 'predictor', 'whose', 'paramet', 'given', 'affin', 'quantil', 'like', 'goal', 'without', 'particular', 'whether', 'subset', 'redund', 'lack', 'deviat', 'version', 'cost', 'convers', 'svms', 'bell', 'boser', 'guyon', 'robust', 'framework', 'vc', 'mark', 'svm', 'assign', 'platt', 'scale', 'probabilist', 'separ', 'implicit', 'creat', 'hava', 'cat', 'lion', 'sum', 'logarithm', 'combin', 'convert', 'outcom', 'proport', 'doe', 'cutoff', 'unlik', 'primarili', 'coin']\n",
      "\n",
      "topic_1 : ['london', 'capit', 'citi', 'england', 'stand', 'river', 'thame', 'head', 'north', 'major', 'settlement', 'two', 'millennia', 'londinium', 'ancient', 'financi', 'area', 'colloqui', 'known', 'close', 'limit', 'adjac', 'westminst', 'borough', 'centuri', 'govern', 'thirti', 'one', 'also', 'compris', 'world', 'import', 'call', 'influenti', 'visit', 'consider', 'impact', 'upon', 'educ', 'entertain', 'financ', 'media', 'profession', 'servic', 'research', 'tourism', 'transport', 'perform', 'either', 'gdp', 'intern', 'arriv', 'busiest', 'airport', 'system', 'traffic', 'destin', 'retail', 'ani', 'number', 'europ', 'highest', 'ultra', 'concentr', 'higher', 'home', 'appli', 'scienc', 'school', 'social', 'three', 'summer', 'olymp', 'divers', 'peopl', 'cultur', 'spoken', 'estim', 'correspond', 'account', 'census', 'commut', 'four', 'heritag', 'site', 'kew', 'garden', 'st', 'margaret', 'church', 'histor', 'greenwich', 'royal', 'meridian', 'longitud', 'mean', 'time', 'landmark', 'piccadilli', 'cathedr', 'bridg', 'trafalgar', 'shard', 'event', 'histori', 'tate', 'end', 'theatr', 'berlin', 'listen', 'decemb', 'make', 'surround', 'potsdam', 'center', 'bank', 'spree', 'flow', 'among', 'main', 'featur', 'lake', 'dahm', 'influenc', 'temper', 'compos', 'forest', 'canal', 'lie', 'central', 'dialect', 'cross', 'rout', 'occup', 'victori', 'countri', 'exclav', 'declar', 'bonn', 'polit', 'economi', 'encompass', 'creativ', 'corpor', 'convent', 'venu', 'continent', 'hub', 'public', 'metropoli', 'popular', 'biotechnolog', 'humboldt', 'hu', 'tu', 'der', 'künste', 'esmt', 'law', 'worldwid', 'studio', 'increas', 'film', 'well', 'festiv', 'architectur', 'contemporari', 'veri', 'sinc', 'cosmopolitan', 'entrepreneuri', 'island', 'hous', 'estat', 'platz', 'memori', 'televis', 'structur', 'jewish', 'forum', 'open', 'late', 'philharmon', 'us', 'romanian', 'bukuˈreʃtʲ', 'romania', 'southeast', 'dâmbovița', 'border', 'neoclass', 'bauhaus', 'communist', 'period', 'parisul', 'estului', 'littl', 'micul', 'heavili', 'destroy', 'even', 'nicola', 'ceaușescu', 'surviv', 'year', 'startup', 'reach', 'valuat', 'summit', 'blockchain', 'list', 'endang', 'monument', 'watch', 'growth', 'stay', 'night', 'index', 'consecut', 'potenti', 'around', 'propos', 'would', 'dure', 'pandem', 'basi', 'infect', 'fourth', 'madrid', 'larg', 'tradit', 'arcad', 'recreat', 'proper', 'municipiul', 'level', 'subdivid', 'local', 'spanish', 'maˈðɾið', 'spain', 'almost', 'eu', 'surpass', 'onli', 'cover', 'josé', 'parti', 'footbal', 'club', 'real', 'atlético', 'standard', 'market', 'size', 'consid', 'offic', 'iag', 'repsol', 'liveabl', 'monocl', 'headquart', 'un', 'unwto', 'general', 'segib', 'interest', 'oversight', 'piob', 'committe', 'academi', 'rae', 'instituto', 'cervant', 'foundat', 'urgent', 'bbva', 'fair', 'infrastructur', 'preserv', 'look', 'plaza', 'retiro', 'triangl', 'along', 'paseo', 'prado', 'complement', 'cibel', 'fountain', 'symbol', 'liʒˈboɐ', 'extend', 'beyond', 'repres', 'westernmost', 'coast', 'tagus', 'portion', 'riviera', 'culmin', 'roca', 'recognis', 'porto', 'contin', 'humberto', 'delgado', 'pendular', 'link', 'istanbul', 'thus', 'place', 'predat', 'julius', 'caesar', 'made', 'municipium', 'felicita', 'name', 'olissipo', 'rule', 'seri', 'tribe', 'moor', 'thoroughfar', 'otherwis', 'travel', 'foot', 'vehicl', 'bicycl', 'hors', 'consist', 'roadway', 'english', 'lane', 'sidewalk', 'pavement', 'path', 'parallel', 'avenu', 'expressway', 'tollway', 'interst', 'highway', 'tertiari', 'vietnames', 'việt', 'vîət', 'nāːm', 'offici', 'socialist', 'hòa', 'xã', 'hội', 'easternmost', 'cambodia', 'maritim', 'indonesia', 'malaysia', 'hanoi', 'ho', 'chi', 'minh', 'saigon', 'archaeolog', 'excav', 'paleolith', 'age', 'red', 'valley', 'nearbi', 'han', 'millennium', 'independ', 'monarchi', 'success', 'expand', 'southward', 'proclam', 'conflict', 'support', 'unitari', 'integr', 'nevertheless', 'face', 'corrupt', 'poverti', 'right', 'record', 'religi', 'group', 'civil', 'establish', 'relat', 'cooper', 'apec', 'sofia', 'софия', 'sofiya', 'ipa', 'ˈsɔfijɐ', 'bulgaria', 'situat', 'mountain', 'part', 'iskar', 'miner', 'spring', 'bath', 'balkan', 'aegean', 'serdica', 'middl', 'habit', 'least', 'attest', 'conquest', 'celtic', 'declin', 'hun', 'incorpor', 'reborn', 'ottoman', 'rumelia', 'eyalet', 'key', 'select', 'usher', 'intens', 'demograph', 'mountainsid', 'andorra', 'commerci', 'describ', 'templ', 'islam', 'banya', 'mosqu', 'ten', 'best', 'busi', 'inform', 'technolog', 'deconstruct', 'second', 'patrimoni', 'orthodox', 'sculptur', 'poster', 'svoge', 'elin', 'pelin', 'radomir', 'stretch', 'machin', 'agent', 'ought', 'take', 'action', 'order', 'reward', 'differ', 'label', 'present', 'correct', 'instead', 'explor', 'unchart', 'exploit', 'knowledg', 'typic', 'process', 'mdp', 'algorithm', 'techniqu', 'latter', 'assum', 'mathemat', 'model', 'map', 'exampl', 'train', 'object', 'vector', 'valu', 'supervisori', 'signal', 'produc', 'new', 'optim', 'scenario', 'determin', 'class', 'unseen', 'requir', 'reason', 'see', 'induct', 'bias', 'psycholog', 'concept', 'previous', 'pattern', 'probabl', 'princip', 'cluster', 'analysi', 'dataset', 'relationship', 'classifi', 'respond', 'feedback', 'piec', 'approach', 'help', 'detect', 'anomal', 'fit', 'minimis', 'loss', 'therefor', 'covari', 'eigenvector', 'similar', 'pass', 'multipl', 'distanc', 'autoencod', 'statist', 'domain', 'summar', 'explain', 'wherea', 'intend', 'priori', 'effici', 'represent', 'encod', 'dimension', 'reduct', 'learnt', 'tri', 'sever', 'forc', 'properti', 'denois', 'contract', 'classif', 'variat', 'acquir', 'semant', 'word', 'partit', 'observ', 'belong', 'nearest', 'centroid', 'prototyp', 'minim', 'euclidean', 'weber', 'geometr', 'median', 'heurist', 'quick', 'mixtur', 'gaussian', 'via', 'iter', 'refin', 'compar', 'mechan', 'shape', 'loos', 'obtain', 'regress', 'respons', 'variabl', 'case', 'term', 'distinct', 'predict', 'singl', 'unknown', 'joint', 'studi', 'rigor', 'practic', 'easier', 'fall', 'broad', 'forecast', 'collect', 'accompani', 'quantifi', 'strength', 'norm', 'absolut', 'penal', 'ridg', 'penalti', 'lasso', 'synonym', 'laboratori', 'vapnik', 'colleagu', 'et', 'theori', 'chervonenki', 'binari', 'clear', 'gap', 'wide', 'kernel', 'trick', 'unlabel', 'attempt', 'siegelmann', 'vladimir', 'logist', 'logit', 'certain', 'imag', 'dog', 'etc', 'odd', 'continu', 'vari', 'altern', 'analog', 'probit', 'constant', 'ratio', 'multinomi', 'ordin', 'simpli', 'choos', 'coeffici', 'express', 'berkson']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = lda_model.getRepresentativeWordsForTopics(show_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, test similarity between cities1 and cities2 (the score similarity should be low as they belong to the topic capitals). Both strategies are used (Jensen-Shannon and KL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score is 0.035019763492461664 (using Jensen-Shannon)\n",
      "\n",
      "Similarity score is 0.13138550494633527 (using KL)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_distribution_doc_1 = lda_model.theta_extracted[0]\n",
    "topic_distribution_doc_2 = lda_model.theta_extracted[1]\n",
    "\n",
    "_ = lda_model.similarity(topic_distribution_doc_1=topic_distribution_doc_1, \n",
    "                         topic_distribution_doc_2=topic_distribution_doc_2, \n",
    "                         type_algorithm=\"Jensen-Shannon\",\n",
    "                         show_similarity=True)\n",
    "\n",
    "_ = lda_model.similarity(topic_distribution_doc_1=topic_distribution_doc_1, \n",
    "                         topic_distribution_doc_2=topic_distribution_doc_2, \n",
    "                         type_algorithm=\"KL\",\n",
    "                         show_similarity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, test similarity between cities1 and science4 (the score similarity should be high as they belong to different topics: cities and ML). Both strategies are used (Jensen-Shannon and KL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score is 0.10556599455301002 (using Jensen-Shannon)\n",
      "\n",
      "Similarity score is 0.40905138530738394 (using KL)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_distribution_doc_1 = lda_model.theta_extracted[0]\n",
    "topic_distribution_doc_2 = lda_model.theta_extracted[11]\n",
    "\n",
    "_ = lda_model.similarity(topic_distribution_doc_1=topic_distribution_doc_1, \n",
    "                         topic_distribution_doc_2=topic_distribution_doc_2, \n",
    "                         type_algorithm=\"Jensen-Shannon\",\n",
    "                         show_similarity=True)\n",
    "\n",
    "_ = lda_model.similarity(topic_distribution_doc_1=topic_distribution_doc_1, \n",
    "                         topic_distribution_doc_2=topic_distribution_doc_2, \n",
    "                         type_algorithm=\"KL\",\n",
    "                         show_similarity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - What about a new document? How can topics be assigned to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same Latent Dirichlet Allocation model can indeed be used to assign topics to unseen documents. The idea is to use the obtained $\\phi$ and $\\theta$ values to create some scores for each topic. In order to achieve that, two methods are implemented:\n",
    "1) $Score(T_i, D_{new}) = P(d_1|T_i) + ... + P(d_{len(D_{new})}|T_i), i=1..no_{topics}.$ (Each word that is not part of the vocabulary is discarded).\n",
    "\n",
    "2) $P(T_i|D_{new}) \\propto P(D_{new}|T_i) * P(T_i) = P(d_1|T_i) * ... * P(d_{len(D_{new}}|T_i) * P(T_i), i=1..no_{topics}. $ (Assuming independece of words with respect to topic $T_i$. The words that are not part of the dictionary are discarded). For numerical stability a log-scale + softmax are used to avoid overflow (so the formula is adjusted accordingly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a new document about Paris from Wikipedia and predict its topics distribution based on the two methods available: heuristics and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_doc = DocumentExtractor(\"Paris\").getContent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print representative words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representative words for each topic:\n",
      "topic_0 : ['largest', 'unit', 'kingdom', 'km', 'estuari', 'lead', 'sea', 'found', 'roman', 'core', 'centr', 'squar', 'mile', 'retain', 'boundari', 'follow', 'mediev', 'inner', 'locat', 'much', 'nation', 'addit', 'south', 'modern', 'mayor', 'global', 'power', 'desir', 'expens', 'sustain', 'exert', 'art', 'commerc', 'fashion', 'healthcar', 'develop', 'rank', 'econom', 'metropolitan', 'measur', 'passeng', 'invest', 'host', 'billionair', 'moscow', 'individu', 'univers', 'form', 'institut', 'high', 'imperi', 'colleg', 'natur', 'becam', 'first', 'rang', 'languag', 'region', 'municip', 'popul', 'greater', 'uk', 'urban', 'third', 'pari', 'inhabit', 'belt', 'contain', 'tower', 'palac', 'abbey', 'observatori', 'defin', 'prime', 'includ', 'buckingham', 'eye', 'circus', 'paul', 'numer', 'museum', 'galleri', 'librari', 'sport', 'british', 'west', 'underground', 'oldest', 'railway', 'network', 'german', 'germani', 'european', 'union', 'accord', 'within', 'feder', 'state', 'brandenburg', 'contigu', 'six', 'million', 'straddl', 'havel', 'tributari', 'elb', 'western', 'spandau', 'topograph', 'mani', 'southeastern', 'müggelse', 'due', 'plain', 'season', 'climat', 'park', 'variant', 'marchian', 'document', 'trade', 'margravi', 'prussia', 'empir', 'weimar', 'republ', 'reich', 'war', 'ii', 'subsequ', 'divid', 'de', 'facto', 'wall', 'east', 'territori', 'reunif', 'onc', 'base', 'firm', 'sector', 'industri', 'facil', 'serv', 'air', 'rail', 'complex', 'tourist', 'signific', 'pharmaceut', 'biomed', 'engin', 'clean', 'tech', 'construct', 'electron', 'universität', 'zu', 'technisch', 'freie', 'free', 'udk', 'zoolog', 'zoo', 'movi', 'product', 'nightlif', 'qualiti', 'live', 'seen', 'emerg', 'gate', 'reichstag', 'build', 'murder', 'jew', 'side', 'column', 'tallest', 'orchestra', 'old', 'bode', 'pergamon', 'schedul', 'opera', 'marathon', 'bucharest', 'bucurești', 'less', 'mi', 'danub', 'bulgarian', 'mention', 'mix', 'nouveau', 'interbellum', 'deco', 'era', 'eleg', 'sophist', 'elit', 'earn', 'nicknam', 'although', 'district', 'damag', 'earthquak', 'program', 'systemat', 'renov', 'recent', 'experienc', 'boom', 'cbre', 'techcrunch', 'uipath', 'billion', 'mastercard', 'past', 'decreas', 'ad', 'satellit', 'town', 'use', 'report', 'rate', 'rome', 'prosper', 'shop', 'administr', 'counti', 'approxim', 'monocentr', 'sq', 'manzanar', 'communiti', 'seat', 'resid', 'monarch', 'lui', 'agglomer', 'environ', 'contribut', 'status', 'output', 'iberian', 'peninsula', 'southern', 'vast', 'compani', 'telefónica', 'magazin', 'organ', 'secretariat', 'oei', 'board', 'regul', 'promot', 'associ', 'fundéu', 'organis', 'fitur', 'arco', 'simo', 'tci', 'possess', 'feel', 'neighbourhood', 'street', 'restor', 'buen', 'archiv', 'golden', 'del', 'reina', 'sofía', 'hold', 'becom', 'lisbon', 'portugues', 'lisboa', 'portug', 'mainland', 'atlant', 'ocean', 'metro', 'point', 'cabo', 'da', 'becaus', 'alongsid', 'grow', 'port', 'motorway', 'alfa', 'barcelona', 'milan', 'athen', 'venic', 'florenc', 'ppp', 'per', 'capita', 'amount', 'occupi', 'gross', 'multin', 'julia', 'captur', 'crusad', 'afonso', 'henriqu', 'reconqu', 'road', 'way', 'land', 'pave', 'improv', 'allow', 'convey', 'motor', 'cart', 'carriageway', 'verg', 'bike', 'may', 'parkway', 'freeway', 'primari', 'secondari', 'vietnam', 'nam', 'cộng', 'chủ', 'nghĩa', 'asia', 'indochines', 'share', 'china', 'lao', 'thailand', 'gulf', 'philippin', 'former', 'indic', 'earli', 'coastal', 'annex', 'dynasti', 'bc', 'divis', 'indochina', 'colonis', 'french', 'born', 'franc', 'rival', 'intensifi', 'saw', 'extens', 'intervent', 'reunifi', 'isol', 'initi', 'reform', 'facilit', 'result', 'enjoy', 'challeng', 'pollut', 'inadequ', 'welfar', 'poor', 'human', 'persecut', 'advoc', 'restrict', 'liberti', 'diplomat', 'member', 'asian', 'asean', 'wto', 'eponym', 'vitosha', 'built', 'humid', 'midway', 'black', 'adriat', 'closest', 'antiqu', 'sredet', 'begin', 'serdi', 'raid', 'visigoth', 'avar', 'slav', 'khan', 'krum', 'byzantin', 'reincorpor', 'literari', 'provinc', 'next', 'lyulin', 'la', 'vella', 'primat', 'toler', 'fact', 'sveta', 'nedelya', 'bashi', 'synagogu', 'cathol', 'joseph', 'top', 'especi', 'afford', 'boyana', 'onto', 'ballet', 'vasil', 'levski', 'stadium', 'ivan', 'vazov', 'amphitheatr', 'visitor', 'lifestyl', 'began', 'percentil', 'dragoman', 'slivnitsa', 'kostinbrod', 'bozhurisht', 'gorna', 'malina', 'ihtiman', 'kostenet', 'pernik', 'hour', 'car', 'dimitrovgrad', 'serbia', 'reinforc', 'learn', 'rl', 'concern', 'softwar', 'maxim', 'notion', 'cumul', 'basic', 'paradigm', 'supervis', 'unsupervis', 'need', 'pair', 'explicit', 'focus', 'find', 'balanc', 'current', 'markov', 'decis', 'context', 'dynam', 'classic', 'method', 'exact', 'target', 'mdps', 'infeas', 'task', 'function', 'input', 'infer', 'data', 'set', 'analyz', 'instanc', 'anim', 'often', 'refer', 'type', 'undetect', 'minimum', 'contrast', 'usual', 'densiti', 'categori', 'compon', 'segment', 'attribut', 'extrapol', 'branch', 'categor', 'identifi', 'common', 'react', 'presenc', 'absenc', 'strategi', 'space', 'characterist', 'origin', 'generat', 'matrix', 'linear', 'algebra', 'eigendecomposit', 'oper', 'varianc', 'sigmoid', 'applic', 'field', 'though', 'involv', 'could', 'say', 'condit', 'distribut', 'adversari', 'artifici', 'neural', 'code', 'manner', 'aim', 'ignor', 'nois', 'reconstruct', 'reduc', 'possibl', 'henc', 'exist', 'regular', 'spars', 'proven', 'effect', 'solv', 'problem', 'recognit', 'quantize', 'voronoi', 'cell', 'difficult', 'error', 'better', 'solut', 'comput', 'howev', 'converg', 'optimum', 'employ', 'tend', 'spatial', 'extent', 'neighbor', 'confus', 'rocchio', 'scalar', 'explanatori', 'depend', 'simpl', 'multivari', 'correl', 'rather', 'predictor', 'whose', 'paramet', 'given', 'affin', 'quantil', 'like', 'goal', 'without', 'particular', 'whether', 'subset', 'redund', 'lack', 'deviat', 'version', 'cost', 'convers', 'svms', 'bell', 'boser', 'guyon', 'robust', 'framework', 'vc', 'mark', 'svm', 'assign', 'platt', 'scale', 'probabilist', 'separ', 'implicit', 'creat', 'hava', 'cat', 'lion', 'sum', 'logarithm', 'combin', 'convert', 'outcom', 'proport', 'doe', 'cutoff', 'unlik', 'primarili', 'coin']\n",
      "\n",
      "topic_1 : ['london', 'capit', 'citi', 'england', 'stand', 'river', 'thame', 'head', 'north', 'major', 'settlement', 'two', 'millennia', 'londinium', 'ancient', 'financi', 'area', 'colloqui', 'known', 'close', 'limit', 'adjac', 'westminst', 'borough', 'centuri', 'govern', 'thirti', 'one', 'also', 'compris', 'world', 'import', 'call', 'influenti', 'visit', 'consider', 'impact', 'upon', 'educ', 'entertain', 'financ', 'media', 'profession', 'servic', 'research', 'tourism', 'transport', 'perform', 'either', 'gdp', 'intern', 'arriv', 'busiest', 'airport', 'system', 'traffic', 'destin', 'retail', 'ani', 'number', 'europ', 'highest', 'ultra', 'concentr', 'higher', 'home', 'appli', 'scienc', 'school', 'social', 'three', 'summer', 'olymp', 'divers', 'peopl', 'cultur', 'spoken', 'estim', 'correspond', 'account', 'census', 'commut', 'four', 'heritag', 'site', 'kew', 'garden', 'st', 'margaret', 'church', 'histor', 'greenwich', 'royal', 'meridian', 'longitud', 'mean', 'time', 'landmark', 'piccadilli', 'cathedr', 'bridg', 'trafalgar', 'shard', 'event', 'histori', 'tate', 'end', 'theatr', 'berlin', 'listen', 'decemb', 'make', 'surround', 'potsdam', 'center', 'bank', 'spree', 'flow', 'among', 'main', 'featur', 'lake', 'dahm', 'influenc', 'temper', 'compos', 'forest', 'canal', 'lie', 'central', 'dialect', 'cross', 'rout', 'occup', 'victori', 'countri', 'exclav', 'declar', 'bonn', 'polit', 'economi', 'encompass', 'creativ', 'corpor', 'convent', 'venu', 'continent', 'hub', 'public', 'metropoli', 'popular', 'biotechnolog', 'humboldt', 'hu', 'tu', 'der', 'künste', 'esmt', 'law', 'worldwid', 'studio', 'increas', 'film', 'well', 'festiv', 'architectur', 'contemporari', 'veri', 'sinc', 'cosmopolitan', 'entrepreneuri', 'island', 'hous', 'estat', 'platz', 'memori', 'televis', 'structur', 'jewish', 'forum', 'open', 'late', 'philharmon', 'us', 'romanian', 'bukuˈreʃtʲ', 'romania', 'southeast', 'dâmbovița', 'border', 'neoclass', 'bauhaus', 'communist', 'period', 'parisul', 'estului', 'littl', 'micul', 'heavili', 'destroy', 'even', 'nicola', 'ceaușescu', 'surviv', 'year', 'startup', 'reach', 'valuat', 'summit', 'blockchain', 'list', 'endang', 'monument', 'watch', 'growth', 'stay', 'night', 'index', 'consecut', 'potenti', 'around', 'propos', 'would', 'dure', 'pandem', 'basi', 'infect', 'fourth', 'madrid', 'larg', 'tradit', 'arcad', 'recreat', 'proper', 'municipiul', 'level', 'subdivid', 'local', 'spanish', 'maˈðɾið', 'spain', 'almost', 'eu', 'surpass', 'onli', 'cover', 'josé', 'parti', 'footbal', 'club', 'real', 'atlético', 'standard', 'market', 'size', 'consid', 'offic', 'iag', 'repsol', 'liveabl', 'monocl', 'headquart', 'un', 'unwto', 'general', 'segib', 'interest', 'oversight', 'piob', 'committe', 'academi', 'rae', 'instituto', 'cervant', 'foundat', 'urgent', 'bbva', 'fair', 'infrastructur', 'preserv', 'look', 'plaza', 'retiro', 'triangl', 'along', 'paseo', 'prado', 'complement', 'cibel', 'fountain', 'symbol', 'liʒˈboɐ', 'extend', 'beyond', 'repres', 'westernmost', 'coast', 'tagus', 'portion', 'riviera', 'culmin', 'roca', 'recognis', 'porto', 'contin', 'humberto', 'delgado', 'pendular', 'link', 'istanbul', 'thus', 'place', 'predat', 'julius', 'caesar', 'made', 'municipium', 'felicita', 'name', 'olissipo', 'rule', 'seri', 'tribe', 'moor', 'thoroughfar', 'otherwis', 'travel', 'foot', 'vehicl', 'bicycl', 'hors', 'consist', 'roadway', 'english', 'lane', 'sidewalk', 'pavement', 'path', 'parallel', 'avenu', 'expressway', 'tollway', 'interst', 'highway', 'tertiari', 'vietnames', 'việt', 'vîət', 'nāːm', 'offici', 'socialist', 'hòa', 'xã', 'hội', 'easternmost', 'cambodia', 'maritim', 'indonesia', 'malaysia', 'hanoi', 'ho', 'chi', 'minh', 'saigon', 'archaeolog', 'excav', 'paleolith', 'age', 'red', 'valley', 'nearbi', 'han', 'millennium', 'independ', 'monarchi', 'success', 'expand', 'southward', 'proclam', 'conflict', 'support', 'unitari', 'integr', 'nevertheless', 'face', 'corrupt', 'poverti', 'right', 'record', 'religi', 'group', 'civil', 'establish', 'relat', 'cooper', 'apec', 'sofia', 'софия', 'sofiya', 'ipa', 'ˈsɔfijɐ', 'bulgaria', 'situat', 'mountain', 'part', 'iskar', 'miner', 'spring', 'bath', 'balkan', 'aegean', 'serdica', 'middl', 'habit', 'least', 'attest', 'conquest', 'celtic', 'declin', 'hun', 'incorpor', 'reborn', 'ottoman', 'rumelia', 'eyalet', 'key', 'select', 'usher', 'intens', 'demograph', 'mountainsid', 'andorra', 'commerci', 'describ', 'templ', 'islam', 'banya', 'mosqu', 'ten', 'best', 'busi', 'inform', 'technolog', 'deconstruct', 'second', 'patrimoni', 'orthodox', 'sculptur', 'poster', 'svoge', 'elin', 'pelin', 'radomir', 'stretch', 'machin', 'agent', 'ought', 'take', 'action', 'order', 'reward', 'differ', 'label', 'present', 'correct', 'instead', 'explor', 'unchart', 'exploit', 'knowledg', 'typic', 'process', 'mdp', 'algorithm', 'techniqu', 'latter', 'assum', 'mathemat', 'model', 'map', 'exampl', 'train', 'object', 'vector', 'valu', 'supervisori', 'signal', 'produc', 'new', 'optim', 'scenario', 'determin', 'class', 'unseen', 'requir', 'reason', 'see', 'induct', 'bias', 'psycholog', 'concept', 'previous', 'pattern', 'probabl', 'princip', 'cluster', 'analysi', 'dataset', 'relationship', 'classifi', 'respond', 'feedback', 'piec', 'approach', 'help', 'detect', 'anomal', 'fit', 'minimis', 'loss', 'therefor', 'covari', 'eigenvector', 'similar', 'pass', 'multipl', 'distanc', 'autoencod', 'statist', 'domain', 'summar', 'explain', 'wherea', 'intend', 'priori', 'effici', 'represent', 'encod', 'dimension', 'reduct', 'learnt', 'tri', 'sever', 'forc', 'properti', 'denois', 'contract', 'classif', 'variat', 'acquir', 'semant', 'word', 'partit', 'observ', 'belong', 'nearest', 'centroid', 'prototyp', 'minim', 'euclidean', 'weber', 'geometr', 'median', 'heurist', 'quick', 'mixtur', 'gaussian', 'via', 'iter', 'refin', 'compar', 'mechan', 'shape', 'loos', 'obtain', 'regress', 'respons', 'variabl', 'case', 'term', 'distinct', 'predict', 'singl', 'unknown', 'joint', 'studi', 'rigor', 'practic', 'easier', 'fall', 'broad', 'forecast', 'collect', 'accompani', 'quantifi', 'strength', 'norm', 'absolut', 'penal', 'ridg', 'penalti', 'lasso', 'synonym', 'laboratori', 'vapnik', 'colleagu', 'et', 'theori', 'chervonenki', 'binari', 'clear', 'gap', 'wide', 'kernel', 'trick', 'unlabel', 'attempt', 'siegelmann', 'vladimir', 'logist', 'logit', 'certain', 'imag', 'dog', 'etc', 'odd', 'continu', 'vari', 'altern', 'analog', 'probit', 'constant', 'ratio', 'multinomi', 'ordin', 'simpli', 'choos', 'coeffici', 'express', 'berkson']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = lda_model.getRepresentativeWordsForTopics(show_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print topic distribution based on the heuristical approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      "['pari', 'french', 'pronunci', 'paʁi', 'listen', 'capit', 'popul', 'citi', 'franc', 'estim', 'popul', 'resid', 'area', 'squar', 'kilometr', 'squar', 'mile', 'sinc', 'centuri', 'pari', 'one', 'europ', 'major', 'centr', 'financ', 'diplomaci', 'commerc', 'fashion', 'scienc', 'art', 'citi', 'pari', 'centr', 'seat', 'govern', 'pari', 'region', 'estim', 'offici', 'popul', 'percent', 'popul', 'franc', 'pari', 'region', 'gdp', 'billion', 'billion', 'accord', 'economist', 'intellig', 'unit', 'worldwid', 'cost', 'live', 'survey', 'pari', 'second', 'expens', 'citi', 'world', 'singapor', 'ahead', 'zürich', 'hong', 'kong', 'oslo', 'geneva', 'anoth', 'sourc', 'rank', 'pari', 'expens', 'par', 'singapor', 'hong', 'kong', 'citi', 'major', 'railway', 'highway', 'hub', 'serv', 'two', 'intern', 'airport', 'de', 'gaull', 'second', 'busiest', 'airport', 'europ', 'open', 'citi', 'subway', 'system', 'pari', 'métro', 'serv', 'million', 'passeng', 'daili', 'second', 'busiest', 'metro', 'system', 'europ', 'moscow', 'metro', 'gare', 'du', 'nord', 'busiest', 'railway', 'station', 'world', 'first', 'locat', 'outsid', 'japan', 'million', 'passeng', 'pari', 'especi', 'known', 'museum', 'architectur', 'landmark', 'louvr', 'visit', 'art', 'museum', 'world', 'million', 'visitor', 'musé', 'musé', 'marmottan', 'monet', 'musé', 'de', 'note', 'collect', 'french', 'impressionist', 'art', 'pompidou', 'centr', 'musé', 'nation', 'modern', 'largest', 'collect', 'modern', 'contemporari', 'art', 'europ', 'musé', 'rodin', 'musé', 'picasso', 'exhibit', 'work', 'two', 'note', 'parisian', 'histor', 'district', 'along', 'sein', 'citi', 'centr', 'classifi', 'unesco', 'heritag', 'site', 'popular', 'landmark', 'includ', 'cathedr', 'notr', 'dame', 'de', 'pari', 'île', 'de', 'la', 'cité', 'close', 'renov', 'april', 'fire', 'popular', 'tourist', 'site', 'includ', 'gothic', 'royal', 'chapel', 'also', 'île', 'de', 'la', 'cité', 'eiffel', 'tower', 'construct', 'pari', 'univers', 'exposit', 'grand', 'palai', 'petit', 'palai', 'built', 'pari', 'univers', 'exposit', 'arc', 'de', 'triomph', 'basilica', 'hill', 'montmartr', 'pari', 'receiv', 'million', 'visitor', 'measur', 'hotel', 'stay', 'largest', 'number', 'foreign', 'visitor', 'come', 'unit', 'state', 'unit', 'kingdom', 'germani', 'china', 'rank', 'second', 'visit', 'travel', 'destin', 'world', 'bangkok', 'ahead', 'london', 'footbal', 'club', 'pari', 'rugbi', 'union', 'club', 'stade', 'françai', 'base', 'pari', 'stade', 'de', 'franc', 'built', 'fifa', 'world', 'cup', 'locat', 'north', 'pari', 'neighbour', 'commune', 'pari', 'host', 'annual', 'french', 'open', 'grand', 'slam', 'tenni', 'tournament', 'red', 'clay', 'roland', 'garro', 'citi', 'host', 'olymp', 'game', 'host', 'summer', 'olymp', 'fifa', 'world', 'cup', 'rugbi', 'world', 'cup', 'well', 'uefa', 'european', 'championship', 'also', 'held', 'citi', 'everi', 'juli', 'tour', 'de', 'franc', 'bicycl', 'race', 'finish', 'avenu', 'des', 'pari']\n",
      "Topic scores (heuristical approach): \n",
      "Topic_0 score: 0.47491848002482817\n",
      "Topic_1 score: 0.5250815199751718\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = lda_model.predict_heuristic(paris_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print topic distribution based on the statistical approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      "['pari', 'french', 'pronunci', 'paʁi', 'listen', 'capit', 'popul', 'citi', 'franc', 'estim', 'popul', 'resid', 'area', 'squar', 'kilometr', 'squar', 'mile', 'sinc', 'centuri', 'pari', 'one', 'europ', 'major', 'centr', 'financ', 'diplomaci', 'commerc', 'fashion', 'scienc', 'art', 'citi', 'pari', 'centr', 'seat', 'govern', 'pari', 'region', 'estim', 'offici', 'popul', 'percent', 'popul', 'franc', 'pari', 'region', 'gdp', 'billion', 'billion', 'accord', 'economist', 'intellig', 'unit', 'worldwid', 'cost', 'live', 'survey', 'pari', 'second', 'expens', 'citi', 'world', 'singapor', 'ahead', 'zürich', 'hong', 'kong', 'oslo', 'geneva', 'anoth', 'sourc', 'rank', 'pari', 'expens', 'par', 'singapor', 'hong', 'kong', 'citi', 'major', 'railway', 'highway', 'hub', 'serv', 'two', 'intern', 'airport', 'de', 'gaull', 'second', 'busiest', 'airport', 'europ', 'open', 'citi', 'subway', 'system', 'pari', 'métro', 'serv', 'million', 'passeng', 'daili', 'second', 'busiest', 'metro', 'system', 'europ', 'moscow', 'metro', 'gare', 'du', 'nord', 'busiest', 'railway', 'station', 'world', 'first', 'locat', 'outsid', 'japan', 'million', 'passeng', 'pari', 'especi', 'known', 'museum', 'architectur', 'landmark', 'louvr', 'visit', 'art', 'museum', 'world', 'million', 'visitor', 'musé', 'musé', 'marmottan', 'monet', 'musé', 'de', 'note', 'collect', 'french', 'impressionist', 'art', 'pompidou', 'centr', 'musé', 'nation', 'modern', 'largest', 'collect', 'modern', 'contemporari', 'art', 'europ', 'musé', 'rodin', 'musé', 'picasso', 'exhibit', 'work', 'two', 'note', 'parisian', 'histor', 'district', 'along', 'sein', 'citi', 'centr', 'classifi', 'unesco', 'heritag', 'site', 'popular', 'landmark', 'includ', 'cathedr', 'notr', 'dame', 'de', 'pari', 'île', 'de', 'la', 'cité', 'close', 'renov', 'april', 'fire', 'popular', 'tourist', 'site', 'includ', 'gothic', 'royal', 'chapel', 'also', 'île', 'de', 'la', 'cité', 'eiffel', 'tower', 'construct', 'pari', 'univers', 'exposit', 'grand', 'palai', 'petit', 'palai', 'built', 'pari', 'univers', 'exposit', 'arc', 'de', 'triomph', 'basilica', 'hill', 'montmartr', 'pari', 'receiv', 'million', 'visitor', 'measur', 'hotel', 'stay', 'largest', 'number', 'foreign', 'visitor', 'come', 'unit', 'state', 'unit', 'kingdom', 'germani', 'china', 'rank', 'second', 'visit', 'travel', 'destin', 'world', 'bangkok', 'ahead', 'london', 'footbal', 'club', 'pari', 'rugbi', 'union', 'club', 'stade', 'françai', 'base', 'pari', 'stade', 'de', 'franc', 'built', 'fifa', 'world', 'cup', 'locat', 'north', 'pari', 'neighbour', 'commune', 'pari', 'host', 'annual', 'french', 'open', 'grand', 'slam', 'tenni', 'tournament', 'red', 'clay', 'roland', 'garro', 'citi', 'host', 'olymp', 'game', 'host', 'summer', 'olymp', 'fifa', 'world', 'cup', 'rugbi', 'world', 'cup', 'well', 'uefa', 'european', 'championship', 'also', 'held', 'citi', 'everi', 'juli', 'tour', 'de', 'franc', 'bicycl', 'race', 'finish', 'avenu', 'des', 'pari']\n",
      "Topic scores (statistical approach) per topics:\n",
      "\n",
      "Topic_0 score: 0.9999999997703526\n",
      "Topic_1 score: 2.296796972561057e-10\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_proba = np.sum(lda_model.theta_extracted, axis=0) / lda_model.no_documents\n",
    "\n",
    "_ = lda_model.predict_statistics(paris_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the heuristic method is more flexible when classifying new documents, it can be seen that Topic 0 (capitals) is by far the most suitable one, being spotted with high probability by the statistical approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
